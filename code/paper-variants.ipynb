{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T23:47:34.721742Z","iopub.status.busy":"2023-05-01T23:47:34.721096Z","iopub.status.idle":"2023-05-01T23:47:39.386226Z","shell.execute_reply":"2023-05-01T23:47:39.385091Z","shell.execute_reply.started":"2023-05-01T23:47:34.721705Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm, trange\n","import torch\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","from datasets_dataloader_pytorch import CustomDataset, load_data\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n","from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, confusion_matrix\n","from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, average_precision_score\n","\n","from CAMELOT import CamelotModel, class_weight\n","from model_utils import MyLRScheduler, calc_l1_l2_loss\n","from utils import calc_pred_loss, calc_dist_loss, calc_clus_loss, torch_log\n","\n","metrics = ['AUC', 'F1 score', 'Recall', 'NMI']\n","seeds = [1001, 1012, 1134, 2475, 6138, 7415, 1663, 7205, 9253, 1782]"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T23:57:57.664302Z","iopub.status.busy":"2023-05-01T23:57:57.663970Z","iopub.status.idle":"2023-05-02T00:13:51.820218Z","shell.execute_reply":"2023-05-02T00:13:51.819212Z","shell.execute_reply.started":"2023-05-01T23:57:57.664274Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:12<00:00, 632.52it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:14<00:14,  3.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:56<00:00,  1.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.77337, \t[0.87801372 0.78423927 0.75752452 0.67370918]\n","F1-score: \t0.28072, \t[0.         0.47262248 0.65024631 0.        ]\n","Recall: \t0.34648, \t[0.         0.8803681  0.50553191 0.        ]\n","NMI: \t\t0.09244\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:13<00:00, 587.82it/s]\n"," 50%|█████     | 50/100 [00:15<00:15,  3.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:15<00:15,  3.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [01:05<00:00,  1.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.78254, \t[0.88453937 0.78543456 0.74934337 0.71084645]\n","F1-score: \t0.33899, \t[0.         0.50728863 0.84865209 0.        ]\n","Recall: \t0.34610, \t[0.         0.53374233 0.8506383  0.        ]\n","NMI: \t\t0.11785\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:12<00:00, 630.28it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.79506, \t[0.8972068  0.79897557 0.76723754 0.71681118]\n","F1-score: \t0.34045, \t[0.         0.52589641 0.83591872 0.        ]\n","Recall: \t0.35535, \t[0.         0.60736196 0.81404255 0.        ]\n","NMI: \t\t0.11889\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 727.86it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.76984, \t[0.88031689 0.79648584 0.77335099 0.62918821]\n","F1-score: \t0.31292, \t[0.         0.50793651 0.74375624 0.        ]\n","Recall: \t0.36086, \t[0.         0.80981595 0.63361702 0.        ]\n","NMI: \t\t0.10792\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 724.27it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.77188, \t[0.90392029 0.77566572 0.75097826 0.65695633]\n","F1-score: \t0.33523, \t[0.         0.49857955 0.84235294 0.        ]\n","Recall: \t0.34405, \t[0.         0.53834356 0.83787234 0.        ]\n","NMI: \t\t0.10775\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:11<00:00, 697.72it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.70530, \t[0.79433192 0.72972417 0.71580348 0.58132733]\n","F1-score: \t0.29846, \t[0.         0.4903975  0.70343392 0.        ]\n","Recall: \t0.35434, \t[0.         0.84202454 0.57531915 0.        ]\n","NMI: \t\t0.10191\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 722.75it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.78450, \t[0.91149951 0.78422632 0.75711587 0.68514229]\n","F1-score: \t0.29423, \t[0.         0.48701013 0.68992655 0.        ]\n","Recall: \t0.35193, \t[0.         0.84815951 0.55957447 0.        ]\n","NMI: \t\t0.10370\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 710.62it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.77756, \t[0.91736361 0.77453451 0.76048549 0.6578481 ]\n","F1-score: \t0.30760, \t[0.         0.50094162 0.72947714 0.        ]\n","Recall: \t0.35760, \t[0.         0.81595092 0.61446809 0.        ]\n","NMI: \t\t0.10547\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:11<00:00, 689.38it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.78638, \t[0.91034793 0.79900083 0.77334488 0.66282572]\n","F1-score: \t0.34174, \t[0.         0.53725736 0.82969238 0.        ]\n","Recall: \t0.36247, \t[0.         0.65797546 0.79191489 0.        ]\n","NMI: \t\t0.12944\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:11<00:00, 696.55it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:53<00:00,  1.89it/s]"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.76785, \t[0.90296472 0.77128549 0.74813051 0.64903701]\n","F1-score: \t0.32843, \t[0.         0.50537634 0.80834656 0.        ]\n","Recall: \t0.35177, \t[0.         0.64877301 0.75829787 0.        ]\n","NMI: \t\t0.10231\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["results = np.zeros((len(seeds), 4))\n","for index, SEED in enumerate(seeds):\n","    torch.random.manual_seed(SEED)\n","    np.random.seed(SEED)\n","    random.seed(SEED)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    dataset = CustomDataset(time_range=(0, 10))\n","\n","    # Stratified Sampling for train and val\n","    train_idx, test_idx = train_test_split(np.arange(len(dataset)),\n","                                                test_size=0.4,\n","                                                random_state=SEED,\n","                                                shuffle=True,\n","                                                stratify=np.argmax(dataset.y,axis=-1))\n","\n","    # Subset dataset for train and val\n","    train_val_dataset = dataset.get_subset(train_idx)\n","    test_dataset = dataset.get_subset(test_idx)\n","\n","    train_idx,  val_idx = train_test_split(np.arange(len(train_val_dataset)),\n","                                                test_size=0.4,\n","                                                random_state=SEED,\n","                                                shuffle=True,\n","                                                stratify=np.argmax(train_val_dataset.y,axis=-1))\n","\n","    train_dataset = train_val_dataset.get_subset(train_idx)\n","    val_dataset = train_val_dataset.get_subset(val_idx)\n","\n","    train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset)\n","\n","    model = CamelotModel(input_shape=(train_dataset.x.shape[1], train_dataset.x.shape[2]), seed=SEED, num_clusters=10, latent_dim=64)\n","    model = model.to(device)\n","\n","    train_x = torch.tensor(train_dataset.x).to(device)\n","    train_y = torch.tensor(train_dataset.y).to(device)\n","    val_x = torch.tensor(val_dataset.x).to(device)\n","    val_y = torch.tensor(val_dataset.y).to(device)\n","\n","    model.initialize((train_x, train_y), (val_x, val_y))\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    cluster_optim = torch.optim.Adam([model.cluster_rep_set], lr=0.001)\n","\n","    lr_scheduler = MyLRScheduler(optimizer, patience=15, min_lr=0.00001, factor=0.25)\n","    cluster_lr_scheduler = MyLRScheduler(cluster_optim, patience=15, min_lr=0.00001, factor=0.25)\n","\n","    loss_mat = np.zeros((100, 4, 2))\n","\n","    best_loss = 1e5\n","    count = 0\n","    for i in trange(100):\n","        for step, (x_train, y_train) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            cluster_optim.zero_grad()\n","\n","            y_pred, probs = model.forward_pass(x_train)\n","\n","            loss_weights = class_weight(y_train)\n","\n","            common_loss = calc_pred_loss(y_train, y_pred, loss_weights)\n","\n","            enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","             + calc_l1_l2_loss(part=model.Encoder) \n","            enc_loss.backward(retain_graph=True, inputs=list(model.Encoder.parameters()))\n","\n","            idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","            + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n","            idnetifier_loss.backward(retain_graph=True, inputs=list(model.Identifier.parameters()))\n","\n","            pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n","            pred_loss.backward(retain_graph=True, inputs=list(model.Predictor.parameters()))\n","\n","            clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n","            clus_loss.backward(inputs=model.cluster_rep_set)\n","\n","            optimizer.step()\n","            cluster_optim.step()\n","\n","            loss_mat[i, 0, 0] += enc_loss.item()\n","            loss_mat[i, 1, 0] += idnetifier_loss.item()\n","            loss_mat[i, 2, 0] += pred_loss.item()\n","            loss_mat[i, 3, 0] += clus_loss.item()\n","\n","        with torch.no_grad():\n","            for step, (x_val, y_val) in enumerate(val_loader):\n","                y_pred, probs = model.forward_pass(x_val)\n","\n","                loss_weights = class_weight(y_val)\n","\n","                common_loss = calc_pred_loss(y_val, y_pred, loss_weights)\n","\n","                enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","                 + calc_l1_l2_loss(part=model.Encoder) \n","\n","                idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","                + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n","\n","                pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n","\n","                clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n","\n","                loss_mat[i, 0, 1] += enc_loss.item()\n","                loss_mat[i, 1, 1] += idnetifier_loss.item()\n","                loss_mat[i, 2, 1] += pred_loss.item()\n","                loss_mat[i, 3, 1] += clus_loss.item()\n","\n","            if i >= 30:\n","                if loss_mat[i, 0, 1] < best_loss:\n","                    count = 0\n","                    best_loss = loss_mat[i, 0, 1]\n","                    torch.save(model.state_dict(), './best_model')\n","                else:\n","                    count += 1\n","                    if count >= 50:\n","                        model.load_state_dict(torch.load('./best_model'))\n","        lr_scheduler.step(loss_mat[i, 0, 1])\n","        cluster_lr_scheduler.step(loss_mat[i, 0, 1])\n","\n","#     print(calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3]), calc_l1_l2_loss(part=model.Encoder) + calc_l1_l2_loss(layers=[model.Identifier.fc2]))\n","\n","    model.load_state_dict(torch.load('./best_model'))\n","\n","    real, preds = [], []\n","    with torch.no_grad():\n","        for _, (x, y) in enumerate(test_loader):\n","            y_pred, _ = model.forward_pass(x)\n","            preds.extend(list(y_pred.cpu().detach().numpy()))\n","            real.extend(list(y.cpu().detach().numpy()))\n","\n","    auc = roc_auc_score(real, preds, average=None)\n","\n","    labels_true, labels_pred = np.argmax(real, axis=1), np.argmax(preds, axis=1)\n","\n","    # Compute F1\n","    f1 = f1_score(labels_true, labels_pred, average=None)\n","\n","    # Compute Recall\n","    rec = recall_score(labels_true, labels_pred, average=None)\n","\n","    # Compute NMI\n","    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n","\n","    print(f'AUCROC: \\t{auc.mean():.5f}, \\t{auc}')\n","    print(f'F1-score: \\t{f1.mean():.5f}, \\t{f1}')\n","    print(f'Recall: \\t{rec.mean():.5f}, \\t{rec}')\n","    print(f'NMI: \\t\\t{nmi:.5f}')\n","    \n","    results[index, 0] = auc.mean()\n","    results[index, 1] = f1.mean()\n","    results[index, 2] = rec.mean()\n","    results[index, 3] = nmi"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T00:14:04.670813Z","iopub.status.busy":"2023-05-02T00:14:04.670428Z","iopub.status.idle":"2023-05-02T00:14:04.677223Z","shell.execute_reply":"2023-05-02T00:14:04.676150Z","shell.execute_reply.started":"2023-05-02T00:14:04.670780Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC: 0.771 (0.023)\n","F1 score: 0.318 (0.021)\n","Recall: 0.353 (0.006)\n","NMI: 0.109 (0.010)\n"]}],"source":["for m, u, std in zip(metrics, results.mean(axis=0), results.std(axis=0)):\n","    print(f'{m}: {u:.3f} ({std:.3f})')"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T00:15:06.703468Z","iopub.status.busy":"2023-05-02T00:15:06.703128Z","iopub.status.idle":"2023-05-02T00:30:32.873535Z","shell.execute_reply":"2023-05-02T00:30:32.872601Z","shell.execute_reply.started":"2023-05-02T00:15:06.703439Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:11<00:00, 662.78it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.76733, \t[0.86754329 0.77836003 0.75103647 0.67239117]\n","F1-score: \t0.33383, \t[0.         0.49459265 0.8407155  0.        ]\n","Recall: \t0.34152, \t[0.         0.52607362 0.84       0.        ]\n","NMI: \t\t0.10798\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 726.57it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.77330, \t[0.8613035  0.78128133 0.74634194 0.7042872 ]\n","F1-score: \t0.28195, \t[0.         0.47196653 0.65583536 0.        ]\n","Recall: \t0.34509, \t[0.         0.86503067 0.51531915 0.        ]\n","NMI: \t\t0.09085\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 709.26it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.79171, \t[0.89135903 0.79909175 0.76810752 0.70826369]\n","F1-score: \t0.33788, \t[0.         0.51111111 0.84040491 0.        ]\n","Recall: \t0.34866, \t[0.         0.56441718 0.83021277 0.        ]\n","NMI: \t\t0.11117\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 718.98it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.75960, \t[0.87720516 0.76079271 0.77068894 0.62970981]\n","F1-score: \t0.32774, \t[0.         0.52421959 0.78674556 0.        ]\n","Recall: \t0.36354, \t[0.         0.74693252 0.70723404 0.        ]\n","NMI: \t\t0.11623\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 708.20it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.76947, \t[0.91386802 0.76688569 0.74496173 0.65214977]\n","F1-score: \t0.32637, \t[0.         0.50293772 0.80255649 0.        ]\n","Recall: \t0.35113, \t[0.         0.65644172 0.74808511 0.        ]\n","NMI: \t\t0.10022\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:11<00:00, 691.61it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.75183, \t[0.90486769 0.74914568 0.73146433 0.62185218]\n","F1-score: \t0.28803, \t[0.         0.46264626 0.68945869 0.        ]\n","Recall: \t0.33868, \t[0.         0.78834356 0.56638298 0.        ]\n","NMI: \t\t0.07525\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 702.60it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.74883, \t[0.83957857 0.75443043 0.73177082 0.66955322]\n","F1-score: \t0.33783, \t[0.         0.49573974 0.85559265 0.        ]\n","Recall: \t0.34078, \t[0.         0.49079755 0.87234043 0.        ]\n","NMI: \t\t0.11604\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 700.60it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.73254, \t[0.85329958 0.73570033 0.71219897 0.6289807 ]\n","F1-score: \t0.26343, \t[0.         0.45153756 0.60216278 0.        ]\n","Recall: \t0.33495, \t[0.         0.88957055 0.45021277 0.        ]\n","NMI: \t\t0.07883\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 707.57it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:53<00:00,  1.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.78649, \t[0.92156158 0.78833882 0.76922461 0.66684708]\n","F1-score: \t0.34197, \t[0.         0.53537285 0.83252105 0.        ]\n","Recall: \t0.36094, \t[0.         0.64417178 0.79957447 0.        ]\n","NMI: \t\t0.13199\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 726.08it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.90it/s]"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.77283, \t[0.88434335 0.78302534 0.74520563 0.67876533]\n","F1-score: \t0.33541, \t[0.         0.50140845 0.84023161 0.        ]\n","Recall: \t0.34491, \t[0.         0.54601227 0.83361702 0.        ]\n","NMI: \t\t0.10777\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["results = np.zeros((len(seeds), 4))\n","for index, SEED in enumerate(seeds):\n","    torch.random.manual_seed(SEED)\n","    np.random.seed(SEED)\n","    random.seed(SEED)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    dataset = CustomDataset(time_range=(0, 10))\n","\n","    # Stratified Sampling for train and val\n","    train_idx, test_idx = train_test_split(np.arange(len(dataset)),\n","                                                test_size=0.4,\n","                                                random_state=SEED,\n","                                                shuffle=True,\n","                                                stratify=np.argmax(dataset.y,axis=-1))\n","\n","    # Subset dataset for train and val\n","    train_val_dataset = dataset.get_subset(train_idx)\n","    test_dataset = dataset.get_subset(test_idx)\n","\n","    train_idx,  val_idx = train_test_split(np.arange(len(train_val_dataset)),\n","                                                test_size=0.4,\n","                                                random_state=SEED,\n","                                                shuffle=True,\n","                                                stratify=np.argmax(train_val_dataset.y,axis=-1))\n","\n","    train_dataset = train_val_dataset.get_subset(train_idx)\n","    val_dataset = train_val_dataset.get_subset(val_idx)\n","\n","    train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset)\n","\n","    model = CamelotModel(input_shape=(train_dataset.x.shape[1], train_dataset.x.shape[2]), seed=SEED, num_clusters=10, latent_dim=64, beta=0)\n","    model = model.to(device)\n","\n","    train_x = torch.tensor(train_dataset.x).to(device)\n","    train_y = torch.tensor(train_dataset.y).to(device)\n","    val_x = torch.tensor(val_dataset.x).to(device)\n","    val_y = torch.tensor(val_dataset.y).to(device)\n","\n","    model.initialize((train_x, train_y), (val_x, val_y))\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    cluster_optim = torch.optim.Adam([model.cluster_rep_set], lr=0.001)\n","\n","    lr_scheduler = MyLRScheduler(optimizer, patience=15, min_lr=0.00001, factor=0.25)\n","    cluster_lr_scheduler = MyLRScheduler(cluster_optim, patience=15, min_lr=0.00001, factor=0.25)\n","\n","    loss_mat = np.zeros((100, 4, 2))\n","\n","    best_loss = 1e5\n","    count = 0\n","    for i in trange(100):\n","        for step, (x_train, y_train) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            cluster_optim.zero_grad()\n","\n","            y_pred, probs = model.forward_pass(x_train)\n","\n","            loss_weights = class_weight(y_train)\n","\n","            common_loss = calc_pred_loss(y_train, y_pred, loss_weights)\n","\n","            enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","             + calc_l1_l2_loss(part=model.Encoder) \n","            enc_loss.backward(retain_graph=True, inputs=list(model.Encoder.parameters()))\n","\n","            idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","            + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n","            idnetifier_loss.backward(retain_graph=True, inputs=list(model.Identifier.parameters()))\n","\n","            pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n","            pred_loss.backward(retain_graph=True, inputs=list(model.Predictor.parameters()))\n","\n","            clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n","            clus_loss.backward(inputs=model.cluster_rep_set)\n","\n","            optimizer.step()\n","            cluster_optim.step()\n","\n","            loss_mat[i, 0, 0] += enc_loss.item()\n","            loss_mat[i, 1, 0] += idnetifier_loss.item()\n","            loss_mat[i, 2, 0] += pred_loss.item()\n","            loss_mat[i, 3, 0] += clus_loss.item()\n","\n","        with torch.no_grad():\n","            for step, (x_val, y_val) in enumerate(val_loader):\n","                y_pred, probs = model.forward_pass(x_val)\n","\n","                loss_weights = class_weight(y_val)\n","\n","                common_loss = calc_pred_loss(y_val, y_pred, loss_weights)\n","\n","                enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","                 + calc_l1_l2_loss(part=model.Encoder) \n","\n","                idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","                + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n","\n","                pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n","\n","                clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n","\n","                loss_mat[i, 0, 1] += enc_loss.item()\n","                loss_mat[i, 1, 1] += idnetifier_loss.item()\n","                loss_mat[i, 2, 1] += pred_loss.item()\n","                loss_mat[i, 3, 1] += clus_loss.item()\n","\n","            if i >= 30:\n","                if loss_mat[i, 0, 1] < best_loss:\n","                    count = 0\n","                    best_loss = loss_mat[i, 0, 1]\n","                    torch.save(model.state_dict(), './best_model')\n","                else:\n","                    count += 1\n","                    if count >= 50:\n","                        model.load_state_dict(torch.load('./best_model'))\n","        lr_scheduler.step(loss_mat[i, 0, 1])\n","        cluster_lr_scheduler.step(loss_mat[i, 0, 1])\n","\n","#     print(calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3]), calc_l1_l2_loss(part=model.Encoder) + calc_l1_l2_loss(layers=[model.Identifier.fc2]))\n","\n","    model.load_state_dict(torch.load('./best_model'))\n","\n","    real, preds = [], []\n","    with torch.no_grad():\n","        for _, (x, y) in enumerate(test_loader):\n","            y_pred, _ = model.forward_pass(x)\n","            preds.extend(list(y_pred.cpu().detach().numpy()))\n","            real.extend(list(y.cpu().detach().numpy()))\n","\n","    auc = roc_auc_score(real, preds, average=None)\n","\n","    labels_true, labels_pred = np.argmax(real, axis=1), np.argmax(preds, axis=1)\n","\n","    # Compute F1\n","    f1 = f1_score(labels_true, labels_pred, average=None)\n","\n","    # Compute Recall\n","    rec = recall_score(labels_true, labels_pred, average=None)\n","\n","    # Compute NMI\n","    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n","\n","    print(f'AUCROC: \\t{auc.mean():.5f}, \\t{auc}')\n","    print(f'F1-score: \\t{f1.mean():.5f}, \\t{f1}')\n","    print(f'Recall: \\t{rec.mean():.5f}, \\t{rec}')\n","    print(f'NMI: \\t\\t{nmi:.5f}')\n","    \n","    results[index, 0] = auc.mean()\n","    results[index, 1] = f1.mean()\n","    results[index, 2] = rec.mean()\n","    results[index, 3] = nmi"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T00:31:27.567195Z","iopub.status.busy":"2023-05-02T00:31:27.566515Z","iopub.status.idle":"2023-05-02T00:31:27.572137Z","shell.execute_reply":"2023-05-02T00:31:27.571249Z","shell.execute_reply.started":"2023-05-02T00:31:27.567159Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC: 0.765 (0.017)\n","F1 score: 0.317 (0.027)\n","Recall: 0.347 (0.009)\n","NMI: 0.104 (0.017)\n"]}],"source":["for m, u, std in zip(metrics, results.mean(axis=0), results.std(axis=0)):\n","    print(f'{m}: {u:.3f} ({std:.3f})')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T00:33:17.502324Z","iopub.status.busy":"2023-05-02T00:33:17.501967Z","iopub.status.idle":"2023-05-02T00:48:42.478518Z","shell.execute_reply":"2023-05-02T00:48:42.477713Z","shell.execute_reply.started":"2023-05-02T00:33:17.502294Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:12<00:00, 640.32it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.75846, \t[0.86568115 0.77298751 0.75408039 0.64108964]\n","F1-score: \t0.29019, \t[0.         0.47993095 0.68082847 0.        ]\n","Recall: \t0.34957, \t[0.         0.85276074 0.54553191 0.        ]\n","NMI: \t\t0.09296\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:11<00:00, 692.29it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.76823, \t[0.8677883  0.75090831 0.74081293 0.7134292 ]\n","F1-score: \t0.32112, \t[0.         0.4909621  0.79349817 0.        ]\n","Recall: \t0.34579, \t[0.         0.64570552 0.73744681 0.        ]\n","NMI: \t\t0.09323\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:11<00:00, 698.65it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.78961, \t[0.88619732 0.79715642 0.76605815 0.70903487]\n","F1-score: \t0.33811, \t[0.         0.50681981 0.845629   0.        ]\n","Recall: \t0.34631, \t[0.         0.54141104 0.84382979 0.        ]\n","NMI: \t\t0.11115\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 731.20it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.76238, \t[0.88584613 0.76743124 0.75367494 0.64255067]\n","F1-score: \t0.32349, \t[0.         0.51809124 0.77586207 0.        ]\n","Recall: \t0.36176, \t[0.         0.75766871 0.6893617  0.        ]\n","NMI: \t\t0.11198\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 726.84it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.77184, \t[0.91246325 0.77239491 0.74987746 0.6526265 ]\n","F1-score: \t0.32470, \t[0.         0.50343249 0.79538639 0.        ]\n","Recall: \t0.35212, \t[0.         0.67484663 0.73361702 0.        ]\n","NMI: \t\t0.10018\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 721.34it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.74490, \t[0.87101437 0.74903076 0.73178479 0.62778046]\n","F1-score: \t0.31412, \t[0.         0.49534643 0.76114726 0.        ]\n","Recall: \t0.35164, \t[0.         0.73466258 0.67191489 0.        ]\n","NMI: \t\t0.09452\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 729.75it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.75511, \t[0.86612218 0.75408566 0.73148441 0.66873437]\n","F1-score: \t0.33747, \t[0.         0.49041534 0.85944939 0.        ]\n","Recall: \t0.33857, \t[0.         0.4708589  0.88340426 0.        ]\n","NMI: \t\t0.11696\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 728.05it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.78484, \t[0.93335511 0.77795118 0.76374945 0.6643148 ]\n","F1-score: \t0.33817, \t[0.         0.53524492 0.81741892 0.        ]\n","Recall: \t0.36348, \t[0.         0.68711656 0.76680851 0.        ]\n","NMI: \t\t0.13032\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 724.72it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.78245, \t[0.92308069 0.78952907 0.77596793 0.64122705]\n","F1-score: \t0.33205, \t[0.         0.52249135 0.80570246 0.        ]\n","Recall: \t0.36008, \t[0.         0.69478528 0.74553191 0.        ]\n","NMI: \t\t0.11790\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:11<00:00, 689.16it/s]\n"," 50%|█████     | 50/100 [00:13<00:13,  3.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:52<00:00,  1.89it/s]"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.76318, \t[0.90655831 0.75849588 0.72001746 0.6676463 ]\n","F1-score: \t0.31040, \t[0.         0.51346274 0.66734694 0.0608    ]\n","Recall: \t0.37687, \t[0.         0.62883436 0.55659574 0.3220339 ]\n","NMI: \t\t0.09700\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["results = np.zeros((len(seeds), 4))\n","for index, SEED in enumerate(seeds):\n","    torch.random.manual_seed(SEED)\n","    np.random.seed(SEED)\n","    random.seed(SEED)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    dataset = CustomDataset(time_range=(0, 10))\n","\n","    # Stratified Sampling for train and val\n","    train_idx, test_idx = train_test_split(np.arange(len(dataset)),\n","                                                test_size=0.4,\n","                                                random_state=SEED,\n","                                                shuffle=True,\n","                                                stratify=np.argmax(dataset.y,axis=-1))\n","\n","    # Subset dataset for train and val\n","    train_val_dataset = dataset.get_subset(train_idx)\n","    test_dataset = dataset.get_subset(test_idx)\n","\n","    train_idx,  val_idx = train_test_split(np.arange(len(train_val_dataset)),\n","                                                test_size=0.4,\n","                                                random_state=SEED,\n","                                                shuffle=True,\n","                                                stratify=np.argmax(train_val_dataset.y,axis=-1))\n","\n","    train_dataset = train_val_dataset.get_subset(train_idx)\n","    val_dataset = train_val_dataset.get_subset(val_idx)\n","\n","    train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset)\n","\n","    model = CamelotModel(input_shape=(train_dataset.x.shape[1], train_dataset.x.shape[2]), seed=SEED, num_clusters=10, latent_dim=64, beta=0, alpha=0)\n","    model = model.to(device)\n","\n","    train_x = torch.tensor(train_dataset.x).to(device)\n","    train_y = torch.tensor(train_dataset.y).to(device)\n","    val_x = torch.tensor(val_dataset.x).to(device)\n","    val_y = torch.tensor(val_dataset.y).to(device)\n","\n","    model.initialize((train_x, train_y), (val_x, val_y))\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    cluster_optim = torch.optim.Adam([model.cluster_rep_set], lr=0.001)\n","\n","    lr_scheduler = MyLRScheduler(optimizer, patience=15, min_lr=0.00001, factor=0.25)\n","    cluster_lr_scheduler = MyLRScheduler(cluster_optim, patience=15, min_lr=0.00001, factor=0.25)\n","\n","    loss_mat = np.zeros((100, 4, 2))\n","\n","    best_loss = 1e5\n","    count = 0\n","    for i in trange(100):\n","        for step, (x_train, y_train) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            cluster_optim.zero_grad()\n","\n","            y_pred, probs = model.forward_pass(x_train)\n","\n","            loss_weights = class_weight(y_train)\n","\n","            common_loss = calc_pred_loss(y_train, y_pred, loss_weights)\n","\n","            enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","             + calc_l1_l2_loss(part=model.Encoder) \n","            enc_loss.backward(retain_graph=True, inputs=list(model.Encoder.parameters()))\n","\n","            idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","            + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n","            idnetifier_loss.backward(retain_graph=True, inputs=list(model.Identifier.parameters()))\n","\n","            pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n","            pred_loss.backward(retain_graph=True, inputs=list(model.Predictor.parameters()))\n","\n","            clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n","            clus_loss.backward(inputs=model.cluster_rep_set)\n","\n","            optimizer.step()\n","            cluster_optim.step()\n","\n","            loss_mat[i, 0, 0] += enc_loss.item()\n","            loss_mat[i, 1, 0] += idnetifier_loss.item()\n","            loss_mat[i, 2, 0] += pred_loss.item()\n","            loss_mat[i, 3, 0] += clus_loss.item()\n","\n","        with torch.no_grad():\n","            for step, (x_val, y_val) in enumerate(val_loader):\n","                y_pred, probs = model.forward_pass(x_val)\n","\n","                loss_weights = class_weight(y_val)\n","\n","                common_loss = calc_pred_loss(y_val, y_pred, loss_weights)\n","\n","                enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","                 + calc_l1_l2_loss(part=model.Encoder) \n","\n","                idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","                + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n","\n","                pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n","\n","                clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n","\n","                loss_mat[i, 0, 1] += enc_loss.item()\n","                loss_mat[i, 1, 1] += idnetifier_loss.item()\n","                loss_mat[i, 2, 1] += pred_loss.item()\n","                loss_mat[i, 3, 1] += clus_loss.item()\n","\n","            if i >= 30:\n","                if loss_mat[i, 0, 1] < best_loss:\n","                    count = 0\n","                    best_loss = loss_mat[i, 0, 1]\n","                    torch.save(model.state_dict(), './best_model')\n","                else:\n","                    count += 1\n","                    if count >= 50:\n","                        model.load_state_dict(torch.load('./best_model'))\n","        lr_scheduler.step(loss_mat[i, 0, 1])\n","        cluster_lr_scheduler.step(loss_mat[i, 0, 1])\n","\n","#     print(calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3]), calc_l1_l2_loss(part=model.Encoder) + calc_l1_l2_loss(layers=[model.Identifier.fc2]))\n","\n","    model.load_state_dict(torch.load('./best_model'))\n","\n","    real, preds = [], []\n","    with torch.no_grad():\n","        for _, (x, y) in enumerate(test_loader):\n","            y_pred, _ = model.forward_pass(x)\n","            preds.extend(list(y_pred.cpu().detach().numpy()))\n","            real.extend(list(y.cpu().detach().numpy()))\n","\n","    auc = roc_auc_score(real, preds, average=None)\n","\n","    labels_true, labels_pred = np.argmax(real, axis=1), np.argmax(preds, axis=1)\n","\n","    # Compute F1\n","    f1 = f1_score(labels_true, labels_pred, average=None)\n","\n","    # Compute Recall\n","    rec = recall_score(labels_true, labels_pred, average=None)\n","\n","    # Compute NMI\n","    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n","\n","    print(f'AUCROC: \\t{auc.mean():.5f}, \\t{auc}')\n","    print(f'F1-score: \\t{f1.mean():.5f}, \\t{f1}')\n","    print(f'Recall: \\t{rec.mean():.5f}, \\t{rec}')\n","    print(f'NMI: \\t\\t{nmi:.5f}')\n","    \n","    results[index, 0] = auc.mean()\n","    results[index, 1] = f1.mean()\n","    results[index, 2] = rec.mean()\n","    results[index, 3] = nmi"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T00:48:48.358984Z","iopub.status.busy":"2023-05-02T00:48:48.358610Z","iopub.status.idle":"2023-05-02T00:48:48.368046Z","shell.execute_reply":"2023-05-02T00:48:48.367173Z","shell.execute_reply.started":"2023-05-02T00:48:48.358954Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC: 0.768 (0.013)\n","F1 score: 0.323 (0.014)\n","Recall: 0.355 (0.010)\n","NMI: 0.107 (0.012)\n"]}],"source":["for m, u, std in zip(metrics, results.mean(axis=0), results.std(axis=0)):\n","    print(f'{m}: {u:.3f} ({std:.3f})')"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T00:49:42.290105Z","iopub.status.busy":"2023-05-02T00:49:42.289758Z","iopub.status.idle":"2023-05-02T00:49:42.313517Z","shell.execute_reply":"2023-05-02T00:49:42.312676Z","shell.execute_reply.started":"2023-05-02T00:49:42.290077Z"},"trusted":true},"outputs":[],"source":["from sklearn.cluster import KMeans\n","import numpy as np\n","from tqdm import trange\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","\n","from model_utils import Identifier, Predictor, calc_l1_l2_loss, FeatTimeAttention\n","from utils import calc_pred_loss\n","\n","SEED = 12345\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, input_shape, attention_hidden_dim, latent_dim, dropout):\n","        super().__init__()\n","        self.lstm1 = nn.LSTM(input_size=input_shape[1],\n","                             hidden_size=attention_hidden_dim,\n","                             num_layers=2,\n","                             dropout=dropout,\n","                             batch_first=True)\n","        self.lstm2 = nn.LSTM(input_size=attention_hidden_dim,\n","                             hidden_size=latent_dim,\n","                             num_layers=1,\n","                             batch_first=True)\n","        self.attention = FeatTimeAttention(latent_dim, input_shape)\n","        self.fc = nn.Linear(10*(input_shape[1]+latent_dim), latent_dim)\n","\n","    def forward(self, x):\n","        latent_rep, _ = self.lstm1(x)\n","        latent_rep, _ = self.lstm2(latent_rep)\n","        x_latent = torch.cat((x, latent_rep),dim=2)\n","        x_latent_flat = torch.flatten(x_latent, start_dim=1)\n","        output = self.fc(x_latent_flat)\n","        return output\n","\n","class CamelotModel(nn.Module):\n","    def __init__(self, input_shape, num_clusters=10, latent_dim=128, seed=SEED, output_dim=4,\n","                 alpha=0.01, beta=0.001, regularization=(0.01, 0.01), dropout=0.0,\n","                 cluster_rep_lr=0.001, weighted_loss=True, attention_hidden_dim=16,\n","                 mlp_hidden_dim=30):\n","\n","        super().__init__()\n","        self.seed = seed\n","\n","        self.input_shape = input_shape\n","        self.num_clusters = num_clusters\n","        self.latent_dim = latent_dim\n","        self.output_dim = output_dim\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.regularization = regularization\n","        self.dropout = dropout\n","        self.cluster_rep_lr = cluster_rep_lr\n","        self.weighted_loss = weighted_loss\n","        self.attention_hidden_dim = attention_hidden_dim\n","        self.mlp_hidden_dim = mlp_hidden_dim\n","\n","        # three newtorks\n","        self.Encoder = Encoder(\n","            self.input_shape, self.attention_hidden_dim, self.latent_dim, self.dropout)\n","        self.Identifier = Identifier(\n","            self.latent_dim, self.mlp_hidden_dim, self.dropout, self.num_clusters)\n","        self.Predictor = Predictor(\n","            self.latent_dim, self.mlp_hidden_dim, self.dropout, self.output_dim)\n","\n","        # Cluster Representation params\n","        self.cluster_rep_set = torch.zeros(\n","            size=[self.num_clusters, self.latent_dim], dtype=torch.float32, requires_grad=True)\n","\n","        self.loss_weights = None\n","\n","    def forward(self, x):\n","        z = self.Encoder(x)\n","        probs = self.Identifier(z)\n","        samples = self.get_sample(probs)\n","        representations = self.get_representations(samples)\n","        return self.Predictor(representations)\n","\n","    def forward_pass(self, x):\n","        z = self.Encoder(x)\n","        probs = self.Identifier(z)\n","        # print(probs.shape)\n","        # samples = self.get_sample(probs)\n","        # # print(samples.shape)\n","        # representations = self.get_representations(samples)\n","        # # print(representations.shape)\n","        # return self.Predictor(representations), probs\n","        clus_phens = self.Predictor(self.cluster_rep_set.to(device))\n","        y_pred = torch.matmul(probs, clus_phens)\n","\n","        return y_pred, probs\n","\n","    def get_sample(self, probs):\n","        logits = - torch.log(probs.reshape(-1, self.num_clusters))\n","        samples = torch.multinomial(logits, num_samples=1)\n","        return samples.squeeze()\n","\n","    def get_representations(self, samples):\n","        mask = F.one_hot(samples, num_classes=self.num_clusters).to(\n","            torch.float32)\n","        return torch.matmul(mask.to(device), self.cluster_rep_set.to(device))\n","\n","    def calc_pis(self, X):\n","        return self.Identifier(self.Encoder(X)).numpy()\n","\n","    def get_cluster_reps(self):\n","        return self.cluster_rep_set.numpy()\n","\n","    def assignment(self, X):\n","        pi = self.Identifier(self.Encoder(X)).numpy()\n","        return torch.argmax(pi, dim=1)\n","\n","    def compute_cluster_phenotypes(self):\n","        return self.Predictor(self.cluster_rep_set).numpy()\n","\n","    # def compute_unnorm_attention_weights(self, inputs):\n","    #     # no idea\n","    #     return self.Encoder.compute_unnorm_scores(inputs, cluster_reps=self.cluster_rep_set)\n","\n","    # def compute_norm_attention_weights(self, inputs):\n","    #     # no idea\n","    #     return self.Encoder.compute_norm_scores(inputs, cluster_reps=self.cluster_rep_set)\n","\n","    def initialize(self, train_data, val_data):\n","        x_train, y_train = train_data\n","        x_val, y_val = val_data\n","        self.loss_weights = class_weight(y_train)\n","\n","        # initialize encoder\n","        self.initialize_encoder(x_train, y_train, x_val, y_val)\n","\n","        # initialize cluster\n","        clus_train, clus_val = self.initialize_cluster(x_train, x_val)\n","        self.clus_train = clus_train\n","        self.x_train = x_train\n","\n","        # initialize identifier\n","        self.initialize_identifier(x_train, clus_train, x_val, clus_val)\n","\n","    def initialize_encoder(self, x_train, y_train, x_val, y_val, epochs=100, batch_size=64):\n","        temp = DataLoader(\n","            dataset=TensorDataset(x_train, y_train),\n","            shuffle=True,\n","            batch_size=batch_size\n","        )\n","\n","        iden_loss = torch.full((epochs,), float('nan'))\n","        initialize_optim = torch.optim.Adam(\n","            self.Encoder.parameters(), lr=0.001)\n","\n","        for i in trange(epochs):\n","            epoch_loss = 0\n","            for _, (x_batch, y_batch) in enumerate(temp):\n","                initialize_optim.zero_grad()\n","\n","                z = self.Encoder(x_batch)\n","                y_pred = self.Predictor(z)\n","                loss = calc_pred_loss(\n","                    y_batch, y_pred, self.loss_weights) + calc_l1_l2_loss(part=self.Encoder)\n","\n","                loss.backward()\n","                initialize_optim.step()\n","\n","                epoch_loss += loss.item()\n","\n","            with torch.no_grad():\n","                z = self.Encoder(x_val)\n","                y_pred_val = self.Predictor(z)\n","                loss_val = calc_pred_loss(y_val, y_pred_val, self.loss_weights)\n","\n","            iden_loss[i] = loss_val.item()\n","            if torch.le(iden_loss[-50:], loss_val.item() + 0.001).any():\n","                break\n","\n","        print('Encoder initialization done!')\n","\n","    def initialize_cluster(self, x_train, x_val):\n","        z = self.Encoder(x_train).cpu().detach().numpy()\n","        kmeans = KMeans(self.num_clusters, random_state=self.seed, n_init='auto')\n","        kmeans.fit(z)\n","        print('Kmeans initialization done!')\n","\n","        self.cluster_rep_set = torch.tensor(\n","            kmeans.cluster_centers_, dtype=torch.float32, requires_grad=True)\n","        train_cluster = torch.eye(self.num_clusters)[\n","            kmeans.predict(z)]\n","        val_cluster = torch.eye(self.num_clusters)[kmeans.predict(\n","            self.Encoder(x_val).cpu().detach().numpy())]\n","\n","        print('Cluster initialization done!')\n","        return train_cluster, val_cluster\n","\n","    def initialize_identifier(self, x_train, clus_train, x_val, clus_val, epochs=100, batch_size=64):\n","        temp = DataLoader(\n","            dataset=TensorDataset(x_train, clus_train),\n","            shuffle=True,\n","            batch_size=batch_size\n","        )\n","\n","        iden_loss = torch.full((epochs,), float('nan'))\n","        initialize_optim = torch.optim.Adam(\n","            self.Identifier.parameters(), lr=0.001)\n","\n","        for i in trange(epochs):\n","            epoch_loss = 0\n","            for step_, (x_batch, clus_batch) in enumerate(temp):\n","                initialize_optim.zero_grad()\n","\n","                clus_pred = self.Identifier(self.Encoder(x_batch))\n","                loss = calc_pred_loss(clus_batch, clus_pred) + \\\n","                    calc_l1_l2_loss(layers=[self.Identifier.fc2])\n","\n","                loss.backward()\n","                initialize_optim.step()\n","\n","                epoch_loss += loss.item()\n","\n","            with torch.no_grad():\n","                clus_pred_val = self.Identifier(self.Encoder(x_val))\n","                loss_val = calc_pred_loss(clus_val, clus_pred_val)\n","\n","            iden_loss[i] = loss_val.item()\n","            if torch.le(iden_loss[-50:], loss_val.item() + 0.001).any():\n","                break\n","\n","        print('Identifier initialization done!')\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T00:49:48.188458Z","iopub.status.busy":"2023-05-02T00:49:48.188098Z","iopub.status.idle":"2023-05-02T01:03:47.371584Z","shell.execute_reply":"2023-05-02T01:03:47.370673Z","shell.execute_reply.started":"2023-05-02T00:49:48.188428Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 724.94it/s]\n"," 50%|█████     | 50/100 [00:11<00:11,  4.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:10<00:10,  4.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:48<00:00,  2.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.74739, \t[0.82877328 0.77482939 0.7145033  0.67147136]\n","F1-score: \t0.33127, \t[0.         0.52077238 0.80429813 0.        ]\n","Recall: \t0.35776, \t[0.         0.68251534 0.74851064 0.        ]\n","NMI: \t\t0.11608\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:11<00:00, 680.76it/s]\n"," 50%|█████     | 50/100 [00:11<00:11,  4.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:10<00:10,  4.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:50<00:00,  1.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.74947, \t[0.84624306 0.7394545  0.69009023 0.72210288]\n","F1-score: \t0.32191, \t[0.         0.48447961 0.80315315 0.        ]\n","Recall: \t0.34229, \t[0.         0.61042945 0.7587234  0.        ]\n","NMI: \t\t0.08950\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 710.55it/s]\n"," 50%|█████     | 50/100 [00:11<00:11,  4.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:10<00:10,  4.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:49<00:00,  2.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.79067, \t[0.8895214  0.7793479  0.76392875 0.72987919]\n","F1-score: \t0.30577, \t[0.         0.53669222 0.59365347 0.09271523]\n","Recall: \t0.44864, \t[0.         0.75153374 0.44978723 0.59322034]\n","NMI: \t\t0.11039\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 707.15it/s]\n"," 50%|█████     | 50/100 [00:11<00:11,  4.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:10<00:10,  4.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:49<00:00,  2.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.78227, \t[0.9286671  0.77446379 0.78896237 0.63697574]\n","F1-score: \t0.36807, \t[0.22916667 0.49012658 0.75297619 0.        ]\n","Recall: \t0.48457, \t[0.55       0.74233129 0.64595745 0.        ]\n","NMI: \t\t0.13074\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:11<00:00, 689.20it/s]\n"," 50%|█████     | 50/100 [00:11<00:11,  4.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:10<00:10,  4.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:49<00:00,  2.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.74888, \t[0.89362137 0.76307975 0.65459935 0.6842337 ]\n","F1-score: \t0.33317, \t[0.         0.48291572 0.8497692  0.        ]\n","Recall: \t0.33736, \t[0.         0.48773006 0.86170213 0.        ]\n","NMI: \t\t0.10325\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 721.88it/s]\n"," 50%|█████     | 50/100 [00:11<00:11,  4.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:10<00:10,  4.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:49<00:00,  2.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.79460, \t[0.92165959 0.81997912 0.79530314 0.64146541]\n","F1-score: \t0.34689, \t[0.         0.56100478 0.82656994 0.        ]\n","Recall: \t0.37377, \t[0.         0.71932515 0.77574468 0.        ]\n","NMI: \t\t0.15096\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 704.38it/s]\n"," 50%|█████     | 50/100 [00:11<00:11,  4.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:10<00:10,  4.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:49<00:00,  2.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.79979, \t[0.91344332 0.80936385 0.77216637 0.70417223]\n","F1-score: \t0.35637, \t[0.11904762 0.53914067 0.69051322 0.07677543]\n","Recall: \t0.46456, \t[0.25       0.70245399 0.56680851 0.33898305]\n","NMI: \t\t0.12990\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:11<00:00, 680.38it/s]\n"," 50%|█████     | 50/100 [00:11<00:11,  4.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:10<00:10,  4.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:50<00:00,  2.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.77607, \t[0.92494283 0.75360546 0.76150304 0.66422787]\n","F1-score: \t0.31610, \t[0.07174888 0.42314436 0.76948909 0.        ]\n","Recall: \t0.40944, \t[0.4        0.55521472 0.68255319 0.        ]\n","NMI: \t\t0.11068\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 709.67it/s]\n"," 50%|█████     | 50/100 [00:11<00:11,  4.32it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:10<00:10,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:49<00:00,  2.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.51137, \t[0.48918654 0.52599343 0.49838927 0.53191847]\n","F1-score: \t0.21635, \t[0.         0.         0.86540232 0.        ]\n","Recall: \t0.25000, \t[0. 0. 1. 0.]\n","NMI: \t\t0.00000\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:10<00:00, 712.24it/s]\n"," 50%|█████     | 50/100 [00:11<00:11,  4.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:10<00:10,  4.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:49<00:00,  2.02it/s]"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.78165, \t[0.9263476  0.78865138 0.75518235 0.65642632]\n","F1-score: \t0.32128, \t[0.         0.5242047  0.69802956 0.06289308]\n","Recall: \t0.38081, \t[0.         0.58128834 0.60297872 0.33898305]\n","NMI: \t\t0.11034\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["results = np.zeros((len(seeds), 4))\n","for index, SEED in enumerate(seeds):\n","    torch.random.manual_seed(SEED)\n","    np.random.seed(SEED)\n","    random.seed(SEED)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    dataset = CustomDataset(time_range=(0, 10))\n","\n","    # Stratified Sampling for train and val\n","    train_idx, test_idx = train_test_split(np.arange(len(dataset)),\n","                                                test_size=0.4,\n","                                                random_state=SEED,\n","                                                shuffle=True,\n","                                                stratify=np.argmax(dataset.y,axis=-1))\n","\n","    # Subset dataset for train and val\n","    train_val_dataset = dataset.get_subset(train_idx)\n","    test_dataset = dataset.get_subset(test_idx)\n","\n","    train_idx,  val_idx = train_test_split(np.arange(len(train_val_dataset)),\n","                                                test_size=0.4,\n","                                                random_state=SEED,\n","                                                shuffle=True,\n","                                                stratify=np.argmax(train_val_dataset.y,axis=-1))\n","\n","    train_dataset = train_val_dataset.get_subset(train_idx)\n","    val_dataset = train_val_dataset.get_subset(val_idx)\n","\n","    train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset)\n","\n","    model = CamelotModel(input_shape=(train_dataset.x.shape[1], train_dataset.x.shape[2]), seed=SEED, num_clusters=10, latent_dim=64)\n","    model = model.to(device)\n","\n","    train_x = torch.tensor(train_dataset.x).to(device)\n","    train_y = torch.tensor(train_dataset.y).to(device)\n","    val_x = torch.tensor(val_dataset.x).to(device)\n","    val_y = torch.tensor(val_dataset.y).to(device)\n","\n","    model.initialize((train_x, train_y), (val_x, val_y))\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    cluster_optim = torch.optim.Adam([model.cluster_rep_set], lr=0.001)\n","\n","    lr_scheduler = MyLRScheduler(optimizer, patience=15, min_lr=0.00001, factor=0.25)\n","    cluster_lr_scheduler = MyLRScheduler(cluster_optim, patience=15, min_lr=0.00001, factor=0.25)\n","\n","    loss_mat = np.zeros((100, 4, 2))\n","\n","    best_loss = 1e5\n","    count = 0\n","    for i in trange(100):\n","        for step, (x_train, y_train) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            cluster_optim.zero_grad()\n","\n","            y_pred, probs = model.forward_pass(x_train)\n","\n","            loss_weights = class_weight(y_train)\n","\n","            common_loss = calc_pred_loss(y_train, y_pred, loss_weights)\n","\n","            enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","             + calc_l1_l2_loss(part=model.Encoder) \n","            enc_loss.backward(retain_graph=True, inputs=list(model.Encoder.parameters()))\n","\n","            idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","            + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n","            idnetifier_loss.backward(retain_graph=True, inputs=list(model.Identifier.parameters()))\n","\n","            pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n","            pred_loss.backward(retain_graph=True, inputs=list(model.Predictor.parameters()))\n","\n","            clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n","            clus_loss.backward(inputs=model.cluster_rep_set)\n","\n","            optimizer.step()\n","            cluster_optim.step()\n","\n","            loss_mat[i, 0, 0] += enc_loss.item()\n","            loss_mat[i, 1, 0] += idnetifier_loss.item()\n","            loss_mat[i, 2, 0] += pred_loss.item()\n","            loss_mat[i, 3, 0] += clus_loss.item()\n","\n","        with torch.no_grad():\n","            for step, (x_val, y_val) in enumerate(val_loader):\n","                y_pred, probs = model.forward_pass(x_val)\n","\n","                loss_weights = class_weight(y_val)\n","\n","                common_loss = calc_pred_loss(y_val, y_pred, loss_weights)\n","\n","                enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","                 + calc_l1_l2_loss(part=model.Encoder) \n","\n","                idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","                + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n","\n","                pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n","\n","                clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n","\n","                loss_mat[i, 0, 1] += enc_loss.item()\n","                loss_mat[i, 1, 1] += idnetifier_loss.item()\n","                loss_mat[i, 2, 1] += pred_loss.item()\n","                loss_mat[i, 3, 1] += clus_loss.item()\n","\n","            if i >= 30:\n","                if loss_mat[i, 0, 1] < best_loss:\n","                    count = 0\n","                    best_loss = loss_mat[i, 0, 1]\n","                    torch.save(model.state_dict(), './best_model')\n","                else:\n","                    count += 1\n","                    if count >= 50:\n","                        model.load_state_dict(torch.load('./best_model'))\n","        lr_scheduler.step(loss_mat[i, 0, 1])\n","        cluster_lr_scheduler.step(loss_mat[i, 0, 1])\n","\n","#     print(calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3]), calc_l1_l2_loss(part=model.Encoder) + calc_l1_l2_loss(layers=[model.Identifier.fc2]))\n","\n","    model.load_state_dict(torch.load('./best_model'))\n","\n","    real, preds = [], []\n","    with torch.no_grad():\n","        for _, (x, y) in enumerate(test_loader):\n","            y_pred, _ = model.forward_pass(x)\n","            preds.extend(list(y_pred.cpu().detach().numpy()))\n","            real.extend(list(y.cpu().detach().numpy()))\n","\n","    auc = roc_auc_score(real, preds, average=None)\n","\n","    labels_true, labels_pred = np.argmax(real, axis=1), np.argmax(preds, axis=1)\n","\n","    # Compute F1\n","    f1 = f1_score(labels_true, labels_pred, average=None)\n","\n","    # Compute Recall\n","    rec = recall_score(labels_true, labels_pred, average=None)\n","\n","    # Compute NMI\n","    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n","\n","    print(f'AUCROC: \\t{auc.mean():.5f}, \\t{auc}')\n","    print(f'F1-score: \\t{f1.mean():.5f}, \\t{f1}')\n","    print(f'Recall: \\t{rec.mean():.5f}, \\t{rec}')\n","    print(f'NMI: \\t\\t{nmi:.5f}')\n","    \n","    results[index, 0] = auc.mean()\n","    results[index, 1] = f1.mean()\n","    results[index, 2] = rec.mean()\n","    results[index, 3] = nmi"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T01:03:51.290488Z","iopub.status.busy":"2023-05-02T01:03:51.290141Z","iopub.status.idle":"2023-05-02T01:03:51.299541Z","shell.execute_reply":"2023-05-02T01:03:51.298706Z","shell.execute_reply.started":"2023-05-02T01:03:51.290460Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC: 0.748 (0.081)\n","F1 score: 0.322 (0.039)\n","Recall: 0.385 (0.066)\n","NMI: 0.105 (0.039)\n"]}],"source":["for m, u, std in zip(metrics, results.mean(axis=0), results.std(axis=0)):\n","    print(f'{m}: {u:.3f} ({std:.3f})')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1001.04it/s]\n"," 50%|█████     | 50/100 [00:16<00:16,  2.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [01:00<00:00,  1.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.77370, \t[0.88452303 0.77977474 0.76179672 0.6686895 ]\n","F1-score: \t0.29722, \t[0.         0.48690154 0.70199018 0.        ]\n","Recall: \t0.35114, \t[0.         0.82668712 0.57787234 0.        ]\n","NMI: \t\t0.09742\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 982.78it/s] \n"," 50%|█████     | 50/100 [00:15<00:15,  3.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:59<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.77463, \t[0.85921268 0.78093847 0.75035102 0.70802533]\n","F1-score: \t0.28940, \t[0.         0.47944613 0.67815483 0.        ]\n","Recall: \t0.34849, \t[0.         0.84969325 0.54425532 0.        ]\n","NMI: \t\t0.09413\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 995.99it/s] \n"," 50%|█████     | 50/100 [00:15<00:15,  3.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:59<00:00,  1.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.78742, \t[0.88124796 0.78781631 0.7554606  0.72514554]\n","F1-score: \t0.33602, \t[0.         0.52676399 0.81730119 0.        ]\n","Recall: \t0.35901, \t[0.         0.66411043 0.77191489 0.        ]\n","NMI: \t\t0.11556\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1017.82it/s]\n"," 50%|█████     | 50/100 [00:15<00:15,  3.32it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:59<00:00,  1.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.76858, \t[0.87624142 0.79417449 0.77001775 0.63390504]\n","F1-score: \t0.32206, \t[0.         0.51679587 0.77145612 0.        ]\n","Recall: \t0.36193, \t[0.         0.76687117 0.68085106 0.        ]\n","NMI: \t\t0.11125\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1041.67it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:57<00:00,  1.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.76378, \t[0.89913427 0.75607814 0.73282533 0.66706581]\n","F1-score: \t0.32200, \t[0.         0.49510651 0.79291302 0.        ]\n","Recall: \t0.34818, \t[0.         0.6595092  0.73319149 0.        ]\n","NMI: \t\t0.09252\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1040.54it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:57<00:00,  1.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.75221, \t[0.88866384 0.76160599 0.74368222 0.61489753]\n","F1-score: \t0.31467, \t[0.         0.49690722 0.76176684 0.        ]\n","Recall: \t0.35269, \t[0.         0.7392638  0.67148936 0.        ]\n","NMI: \t\t0.09797\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1034.71it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [01:00<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.78143, \t[0.89264946 0.77898956 0.74968216 0.70441059]\n","F1-score: \t0.34016, \t[0.         0.49960349 0.8610535  0.        ]\n","Recall: \t0.34163, \t[0.         0.48312883 0.88340426 0.        ]\n","NMI: \t\t0.12445\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 993.56it/s] \n"," 50%|█████     | 50/100 [00:15<00:15,  3.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:13<00:13,  3.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:59<00:00,  1.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.78318, \t[0.92945933 0.78328707 0.75989114 0.66009714]\n","F1-score: \t0.30070, \t[0.         0.49343594 0.70934435 0.        ]\n","Recall: \t0.35514, \t[0.         0.83588957 0.58468085 0.        ]\n","NMI: \t\t0.10432\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1037.89it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:58<00:00,  1.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.78028, \t[0.92406076 0.77494968 0.7697037  0.65241057]\n","F1-score: \t0.31640, \t[0.         0.51233959 0.75326596 0.        ]\n","Recall: \t0.36156, \t[0.         0.79601227 0.65021277 0.        ]\n","NMI: \t\t0.11482\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1027.89it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n","Kmeans initialization done!\n","Cluster initialization done!\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50/100 [00:12<00:12,  3.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Identifier initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:58<00:00,  1.71it/s]"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.76876, \t[0.88404116 0.77676598 0.74438484 0.66984767]\n","F1-score: \t0.33559, \t[0.         0.50681199 0.83553629 0.        ]\n","Recall: \t0.34774, \t[0.         0.57055215 0.82042553 0.        ]\n","NMI: \t\t0.11063\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["results = np.zeros((len(seeds), 4))\n","for index, SEED in enumerate(seeds):\n","    torch.random.manual_seed(SEED)\n","    np.random.seed(SEED)\n","    random.seed(SEED)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    dataset = CustomDataset(time_range=(0, 10))\n","\n","    # Stratified Sampling for train and val\n","    train_idx, test_idx = train_test_split(np.arange(len(dataset)),\n","                                                test_size=0.4,\n","                                                random_state=SEED,\n","                                                shuffle=True,\n","                                                stratify=np.argmax(dataset.y,axis=-1))\n","\n","    # Subset dataset for train and val\n","    train_val_dataset = dataset.get_subset(train_idx)\n","    test_dataset = dataset.get_subset(test_idx)\n","\n","    train_idx,  val_idx = train_test_split(np.arange(len(train_val_dataset)),\n","                                                test_size=0.4,\n","                                                random_state=SEED,\n","                                                shuffle=True,\n","                                                stratify=np.argmax(train_val_dataset.y,axis=-1))\n","\n","    train_dataset = train_val_dataset.get_subset(train_idx)\n","    val_dataset = train_val_dataset.get_subset(val_idx)\n","\n","    train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset)\n","\n","    model = CamelotModel(input_shape=(train_dataset.x.shape[1], train_dataset.x.shape[2]), seed=SEED, num_clusters=10, latent_dim=64, alpha=0)\n","    model = model.to(device)\n","\n","    train_x = torch.tensor(train_dataset.x).to(device)\n","    train_y = torch.tensor(train_dataset.y).to(device)\n","    val_x = torch.tensor(val_dataset.x).to(device)\n","    val_y = torch.tensor(val_dataset.y).to(device)\n","\n","    model.initialize((train_x, train_y), (val_x, val_y))\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    cluster_optim = torch.optim.Adam([model.cluster_rep_set], lr=0.001)\n","\n","    lr_scheduler = MyLRScheduler(optimizer, patience=15, min_lr=0.00001, factor=0.25)\n","    cluster_lr_scheduler = MyLRScheduler(cluster_optim, patience=15, min_lr=0.00001, factor=0.25)\n","\n","    loss_mat = np.zeros((100, 4, 2))\n","\n","    best_loss = 1e5\n","    count = 0\n","    for i in trange(100):\n","        for step, (x_train, y_train) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            cluster_optim.zero_grad()\n","\n","            y_pred, probs = model.forward_pass(x_train)\n","\n","            loss_weights = class_weight(y_train)\n","\n","            common_loss = calc_pred_loss(y_train, y_pred, loss_weights)\n","\n","            enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","             + calc_l1_l2_loss(part=model.Encoder) \n","            enc_loss.backward(retain_graph=True, inputs=list(model.Encoder.parameters()))\n","\n","            idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","            + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n","            idnetifier_loss.backward(retain_graph=True, inputs=list(model.Identifier.parameters()))\n","\n","            pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n","            pred_loss.backward(retain_graph=True, inputs=list(model.Predictor.parameters()))\n","\n","            clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n","            clus_loss.backward(inputs=model.cluster_rep_set)\n","\n","            optimizer.step()\n","            cluster_optim.step()\n","\n","            loss_mat[i, 0, 0] += enc_loss.item()\n","            loss_mat[i, 1, 0] += idnetifier_loss.item()\n","            loss_mat[i, 2, 0] += pred_loss.item()\n","            loss_mat[i, 3, 0] += clus_loss.item()\n","\n","        with torch.no_grad():\n","            for step, (x_val, y_val) in enumerate(val_loader):\n","                y_pred, probs = model.forward_pass(x_val)\n","\n","                loss_weights = class_weight(y_val)\n","\n","                common_loss = calc_pred_loss(y_val, y_pred, loss_weights)\n","\n","                enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","                 + calc_l1_l2_loss(part=model.Encoder) \n","\n","                idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","                + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n","\n","                pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n","\n","                clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n","\n","                loss_mat[i, 0, 1] += enc_loss.item()\n","                loss_mat[i, 1, 1] += idnetifier_loss.item()\n","                loss_mat[i, 2, 1] += pred_loss.item()\n","                loss_mat[i, 3, 1] += clus_loss.item()\n","\n","            if i >= 30:\n","                if loss_mat[i, 0, 1] < best_loss:\n","                    count = 0\n","                    best_loss = loss_mat[i, 0, 1]\n","                    torch.save(model.state_dict(), './best_model')\n","                else:\n","                    count += 1\n","                    if count >= 50:\n","                        model.load_state_dict(torch.load('./best_model'))\n","        lr_scheduler.step(loss_mat[i, 0, 1])\n","        cluster_lr_scheduler.step(loss_mat[i, 0, 1])\n","\n","#     print(calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3]), calc_l1_l2_loss(part=model.Encoder) + calc_l1_l2_loss(layers=[model.Identifier.fc2]))\n","\n","    model.load_state_dict(torch.load('./best_model'))\n","\n","    real, preds = [], []\n","    with torch.no_grad():\n","        for _, (x, y) in enumerate(test_loader):\n","            y_pred, _ = model.forward_pass(x)\n","            preds.extend(list(y_pred.cpu().detach().numpy()))\n","            real.extend(list(y.cpu().detach().numpy()))\n","\n","    auc = roc_auc_score(real, preds, average=None)\n","\n","    labels_true, labels_pred = np.argmax(real, axis=1), np.argmax(preds, axis=1)\n","\n","    # Compute F1\n","    f1 = f1_score(labels_true, labels_pred, average=None)\n","\n","    # Compute Recall\n","    rec = recall_score(labels_true, labels_pred, average=None)\n","\n","    # Compute NMI\n","    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n","\n","    print(f'AUCROC: \\t{auc.mean():.5f}, \\t{auc}')\n","    print(f'F1-score: \\t{f1.mean():.5f}, \\t{f1}')\n","    print(f'Recall: \\t{rec.mean():.5f}, \\t{rec}')\n","    print(f'NMI: \\t\\t{nmi:.5f}')\n","    \n","    results[index, 0] = auc.mean()\n","    results[index, 1] = f1.mean()\n","    results[index, 2] = rec.mean()\n","    results[index, 3] = nmi"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC: 0.773 (0.010)\n","F1 score: 0.317 (0.016)\n","Recall: 0.353 (0.006)\n","NMI: 0.106 (0.010)\n"]}],"source":["for m, u, std in zip(metrics, results.mean(axis=0), results.std(axis=0)):\n","    print(f'{m}: {u:.3f} ({std:.3f})')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from sklearn.cluster import KMeans\n","import numpy as np\n","from tqdm import trange\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","from model_utils import Encoder, Identifier, Predictor, calc_l1_l2_loss\n","from utils import calc_pred_loss\n","\n","SEED = 12345\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","class CamelotModel_NO_CLUSTER(nn.Module):\n","    def __init__(self, input_shape, num_clusters=10, latent_dim=128, seed=SEED, output_dim=4,\n","                 alpha=0.01, beta=0.001, regularization=(0.01, 0.01), dropout=0.0,\n","                 cluster_rep_lr=0.001, weighted_loss=True, attention_hidden_dim=16,\n","                 mlp_hidden_dim=30):\n","\n","        super().__init__()\n","        self.seed = seed\n","\n","        self.input_shape = input_shape\n","        self.num_clusters = num_clusters\n","        self.latent_dim = latent_dim\n","        self.output_dim = output_dim\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.regularization = regularization\n","        self.dropout = dropout\n","        self.cluster_rep_lr = cluster_rep_lr\n","        self.weighted_loss = weighted_loss\n","        self.attention_hidden_dim = attention_hidden_dim\n","        self.mlp_hidden_dim = mlp_hidden_dim\n","\n","        # three newtorks\n","        self.Encoder = Encoder(\n","            self.input_shape, self.attention_hidden_dim, self.latent_dim, self.dropout)\n","        self.Predictor = Predictor(\n","            self.latent_dim, self.mlp_hidden_dim, self.dropout, self.output_dim)\n","\n","        self.loss_weights = None\n","\n","    def forward(self, x):\n","        z = self.Encoder(x)\n","        y_pred = self.Predictor(z)\n","        pi = torch.ones(self.num_clusters) / self.num_clusters\n","        return y_pred, pi\n","\n","    def forward_pass(self, x):\n","        z = self.Encoder(x)\n","        y_pred = self.Predictor(z)\n","        pi = torch.ones(self.num_clusters) / self.num_clusters\n","        return y_pred, pi\n","\n","    def initialize(self, train_data, val_data):\n","        x_train, y_train = train_data\n","        x_val, y_val = val_data\n","        self.loss_weights = class_weight(y_train)\n","\n","        # initialize encoder\n","        self.initialize_encoder(x_train, y_train, x_val, y_val)\n","\n","        self.x_train = x_train\n","\n","    def initialize_encoder(self, x_train, y_train, x_val, y_val, epochs=100, batch_size=64):\n","        temp = DataLoader(\n","            dataset=TensorDataset(x_train, y_train),\n","            shuffle=True,\n","            batch_size=batch_size\n","        )\n","\n","        iden_loss = torch.full((epochs,), float('nan'))\n","        initialize_optim = torch.optim.Adam(\n","            self.Encoder.parameters(), lr=0.001)\n","\n","        for i in trange(epochs):\n","            epoch_loss = 0\n","            for _, (x_batch, y_batch) in enumerate(temp):\n","                initialize_optim.zero_grad()\n","\n","                z = self.Encoder(x_batch)\n","                y_pred = self.Predictor(z)\n","                loss = calc_pred_loss(\n","                    y_batch, y_pred, self.loss_weights) + calc_l1_l2_loss(part=self.Encoder)\n","\n","                loss.backward()\n","                initialize_optim.step()\n","\n","                epoch_loss += loss.item()\n","\n","            with torch.no_grad():\n","                z = self.Encoder(x_val)\n","                y_pred_val = self.Predictor(z)\n","                loss_val = calc_pred_loss(y_val, y_pred_val, self.loss_weights)\n","\n","            iden_loss[i] = loss_val.item()\n","            if torch.le(iden_loss[-50:], loss_val.item() + 0.001).any():\n","                break\n","\n","        print('Encoder initialization done!')\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1013.47it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:42<00:00,  2.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.72731, \t[0.81390069 0.73444031 0.71571237 0.64520073]\n","F1-score: \t0.29130, \t[0.         0.44783465 0.71735374 0.        ]\n","Recall: \t0.32904, \t[0.         0.69785276 0.61829787 0.        ]\n","NMI: \t\t0.06023\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1014.18it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:41<00:00,  2.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.75257, \t[0.83627083 0.77108785 0.74552697 0.65739941]\n","F1-score: \t0.21635, \t[0.         0.         0.86540232 0.        ]\n","Recall: \t0.25000, \t[0. 0. 1. 0.]\n","NMI: \t\t0.00000\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 990.57it/s] \n"," 50%|█████     | 50/100 [00:14<00:14,  3.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:41<00:00,  2.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.77467, \t[0.89645541 0.75941019 0.73978549 0.70303929]\n","F1-score: \t0.32478, \t[0.0962963  0.4178127  0.69806517 0.08695652]\n","Recall: \t0.51118, \t[0.65       0.4892638  0.58340426 0.3220339 ]\n","NMI: \t\t0.10268\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 977.54it/s] \n"," 50%|█████     | 50/100 [00:15<00:15,  3.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:42<00:00,  2.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.75899, \t[0.87572689 0.77485464 0.76352854 0.62183535]\n","F1-score: \t0.31840, \t[0.         0.50896057 0.76464891 0.        ]\n","Recall: \t0.35855, \t[0.         0.76226994 0.67191489 0.        ]\n","NMI: \t\t0.10465\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1019.79it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:41<00:00,  2.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.66578, \t[0.78990526 0.60966573 0.60411794 0.6594185 ]\n","F1-score: \t0.21635, \t[0.         0.         0.86540232 0.        ]\n","Recall: \t0.25000, \t[0. 0. 1. 0.]\n","NMI: \t\t0.00000\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1022.09it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:41<00:00,  2.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.77436, \t[0.91625286 0.77179442 0.76143843 0.64794894]\n","F1-score: \t0.34447, \t[0.07142857 0.51099831 0.79544398 0.        ]\n","Recall: \t0.36822, \t[0.05       0.69478528 0.72808511 0.        ]\n","NMI: \t\t0.11291\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 971.70it/s] \n"," 50%|█████     | 50/100 [00:15<00:15,  3.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:42<00:00,  2.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.71880, \t[0.85428781 0.6756577  0.66338272 0.68186968]\n","F1-score: \t0.21635, \t[0.         0.         0.86540232 0.        ]\n","Recall: \t0.25000, \t[0. 0. 1. 0.]\n","NMI: \t\t0.00000\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1023.96it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:41<00:00,  2.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.77610, \t[0.94170206 0.75881412 0.74227697 0.66161707]\n","F1-score: \t0.33300, \t[0.         0.51456913 0.81744966 0.        ]\n","Recall: \t0.35349, \t[0.         0.63650307 0.77744681 0.        ]\n","NMI: \t\t0.11664\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1020.70it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:41<00:00,  2.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.77395, \t[0.93245671 0.77467563 0.73622901 0.65242459]\n","F1-score: \t0.33685, \t[0.         0.52625153 0.82114736 0.        ]\n","Recall: \t0.35941, \t[0.         0.66104294 0.77659574 0.        ]\n","NMI: \t\t0.12201\n","\n","MIMIC data has been subsettted to the following features: \n"," ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7701/7701 [00:07<00:00, 1021.10it/s]\n"," 50%|█████     | 50/100 [00:14<00:14,  3.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Encoder initialization done!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:41<00:00,  2.42it/s]"]},{"name":"stdout","output_type":"stream","text":["AUCROC: \t0.75995, \t[0.89198791 0.77783436 0.71207614 0.65789297]\n","F1-score: \t0.30750, \t[0.         0.51870324 0.64302477 0.06827881]\n","Recall: \t0.39237, \t[0.         0.63803681 0.52468085 0.40677966]\n","NMI: \t\t0.10551\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["results = np.zeros((len(seeds), 4))\n","for index, SEED in enumerate(seeds):\n","    torch.random.manual_seed(SEED)\n","    np.random.seed(SEED)\n","    random.seed(SEED)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    dataset = CustomDataset(time_range=(0, 10))\n","\n","    # Stratified Sampling for train and val\n","    train_idx, test_idx = train_test_split(np.arange(len(dataset)),\n","                                                test_size=0.4,\n","                                                random_state=SEED,\n","                                                shuffle=True,\n","                                                stratify=np.argmax(dataset.y,axis=-1))\n","\n","    # Subset dataset for train and val\n","    train_val_dataset = dataset.get_subset(train_idx)\n","    test_dataset = dataset.get_subset(test_idx)\n","\n","    train_idx,  val_idx = train_test_split(np.arange(len(train_val_dataset)),\n","                                                test_size=0.4,\n","                                                random_state=SEED,\n","                                                shuffle=True,\n","                                                stratify=np.argmax(train_val_dataset.y,axis=-1))\n","\n","    train_dataset = train_val_dataset.get_subset(train_idx)\n","    val_dataset = train_val_dataset.get_subset(val_idx)\n","\n","    train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset)\n","\n","    model = CamelotModel_NO_CLUSTER(input_shape=(train_dataset.x.shape[1], train_dataset.x.shape[2]), seed=SEED, num_clusters=10, latent_dim=64, alpha=0)\n","    model = model.to(device)\n","\n","    train_x = torch.tensor(train_dataset.x).to(device)\n","    train_y = torch.tensor(train_dataset.y).to(device)\n","    val_x = torch.tensor(val_dataset.x).to(device)\n","    val_y = torch.tensor(val_dataset.y).to(device)\n","\n","    model.initialize((train_x, train_y), (val_x, val_y))\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    lr_scheduler = MyLRScheduler(optimizer, patience=15, min_lr=0.00001, factor=0.25)\n","\n","    loss_mat = np.zeros((100, 4, 2))\n","\n","    best_loss = 1e5\n","    count = 0\n","    for i in trange(100):\n","        for step, (x_train, y_train) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","\n","            y_pred, probs = model.forward_pass(x_train)\n","\n","            loss_weights = class_weight(y_train)\n","\n","            common_loss = calc_pred_loss(y_train, y_pred, loss_weights)\n","\n","            enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","             + calc_l1_l2_loss(part=model.Encoder) \n","            enc_loss.backward(retain_graph=True, inputs=list(model.Encoder.parameters()))\n","\n","            pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n","            pred_loss.backward(retain_graph=True, inputs=list(model.Predictor.parameters()))\n","\n","            optimizer.step()\n","\n","            loss_mat[i, 0, 0] += enc_loss.item()\n","            loss_mat[i, 2, 0] += pred_loss.item()\n","\n","        with torch.no_grad():\n","            for step, (x_val, y_val) in enumerate(val_loader):\n","                y_pred, probs = model.forward_pass(x_val)\n","\n","                loss_weights = class_weight(y_val)\n","\n","                common_loss = calc_pred_loss(y_val, y_pred, loss_weights)\n","\n","                enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n","                 + calc_l1_l2_loss(part=model.Encoder) \n","\n","                pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n","\n","                loss_mat[i, 0, 1] += enc_loss.item()\n","                loss_mat[i, 2, 1] += pred_loss.item()\n","\n","            if i >= 30:\n","                if loss_mat[i, 0, 1] < best_loss:\n","                    count = 0\n","                    best_loss = loss_mat[i, 0, 1]\n","                    torch.save(model.state_dict(), './best_model')\n","                else:\n","                    count += 1\n","                    if count >= 50:\n","                        model.load_state_dict(torch.load('./best_model'))\n","        lr_scheduler.step(loss_mat[i, 0, 1])\n","\n","    model.load_state_dict(torch.load('./best_model'))\n","\n","    real, preds = [], []\n","    with torch.no_grad():\n","        for _, (x, y) in enumerate(test_loader):\n","            y_pred, _ = model.forward_pass(x)\n","            preds.extend(list(y_pred.cpu().detach().numpy()))\n","            real.extend(list(y.cpu().detach().numpy()))\n","\n","    auc = roc_auc_score(real, preds, average=None)\n","\n","    labels_true, labels_pred = np.argmax(real, axis=1), np.argmax(preds, axis=1)\n","\n","    # Compute F1\n","    f1 = f1_score(labels_true, labels_pred, average=None)\n","\n","    # Compute Recall\n","    rec = recall_score(labels_true, labels_pred, average=None)\n","\n","    # Compute NMI\n","    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n","\n","    print(f'AUCROC: \\t{auc.mean():.5f}, \\t{auc}')\n","    print(f'F1-score: \\t{f1.mean():.5f}, \\t{f1}')\n","    print(f'Recall: \\t{rec.mean():.5f}, \\t{rec}')\n","    print(f'NMI: \\t\\t{nmi:.5f}')\n","    \n","    results[index, 0] = auc.mean()\n","    results[index, 1] = f1.mean()\n","    results[index, 2] = rec.mean()\n","    results[index, 3] = nmi"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC: 0.748 (0.033)\n","F1 score: 0.291 (0.051)\n","Recall: 0.342 (0.076)\n","NMI: 0.072 (0.050)\n"]}],"source":["for m, u, std in zip(metrics, results.mean(axis=0), results.std(axis=0)):\n","    print(f'{m}: {u:.3f} ({std:.3f})')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
