{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nimport torch\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, confusion_matrix\nfrom sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, average_precision_score\n\n\nmetrics = ['AUC', 'F1 score', 'Recall', 'NMI']\nseeds = [1001, 1012, 1134, 2475, 6138, 7415, 1663, 7205, 9253, 1782]","metadata":{"execution":{"iopub.status.busy":"2023-05-01T23:47:34.721096Z","iopub.execute_input":"2023-05-01T23:47:34.721742Z","iopub.status.idle":"2023-05-01T23:47:39.386226Z","shell.execute_reply.started":"2023-05-01T23:47:34.721705Z","shell.execute_reply":"2023-05-01T23:47:39.385091Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"#!/usr/bin/env python3\n# INSERT FILE DESCRIPTION\n\n\"\"\"\nUtil functions to run Model. Includes Data loading, etc...\n\"\"\"\nimport os\nfrom typing import List, Union\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport datetime as dt\n\ntqdm.pandas()\n\n\ndef _compute_last_target_id(df: pd.DataFrame, time_col: str = \"intime\", mode: str = \"max\") -> pd.DataFrame:\n    \"\"\"Identify last ids given df according to time given by time_col column. Mode determines min or max.\"\"\"\n    if mode == \"max\":\n        time = df[time_col].max()\n    elif mode == \"min\":\n        time = df[time_col].min()\n    else:\n        raise ValueError(\"mode must be one of ['min', 'max']. Got {}\".format(mode))\n\n    last_ids = df[df[time_col] == time]\n\n    return last_ids\n\n\ndef _rows_are_in(df1: pd.DataFrame, df2: pd.DataFrame, matching_columns: Union[List[str], str]) -> pd.DataFrame:\n    \"\"\"\n    Checks if values present in row of df1 exist for all columns in df2. Note that this does not necessarily mean\n    the whole row of df1 is in df2, but is good enough for application.\n\n    Returns: array of indices indicating the relevant rows of df1.\n    \"\"\"\n    if isinstance(matching_columns, str):\n        matching_columns = [matching_columns]\n\n    # Iterate through each column\n    matching_ids = np.ones(df1.shape[0])\n    for col in tqdm(matching_columns):\n        col_matching = df1[col].isin(df2[col].values).values  # where df1 col is subset of df2 col\n        matching_ids = np.logical_and(matching_ids, col_matching)  # match with columns already looked at\n\n    return matching_ids\n\n\ndef _compute_second_transfer_info(df: pd.DataFrame, time_col, target_cols):\n    \"\"\"\n    Given transfer data for a unique id, compute the second transfer as given by time_col.\n\n    return: pd.Series with corresponding second transfer info.\n    \"\"\"\n    time_info = df[time_col]\n    second_transfer_time = time_info[time_info != time_info.min()].min()\n\n    # Identify second transfer info - can be empty, unique, or repeated instances\n    second_transfer = df[df[time_col] == second_transfer_time]\n\n    if second_transfer.empty:\n        output = [df.name, df[\"hadm_id\"].iloc[0], df[\"transfer_id\"].iloc[0]] + [np.nan] * (len(target_cols) - 3)\n        return pd.Series(data=output, index=target_cols)\n\n    elif second_transfer.shape[0] == 1:\n        return pd.Series(data=second_transfer.squeeze().values, index=target_cols)\n\n    else:  # There should be NONE\n        print(second_transfer)\n        raise ValueError(\"Something's gone wrong! No expected repeated second transfers with the same time.\")\n\n\ndef convert_columns_to_dt(df: pd.DataFrame, columns: Union[str, List[str]]):\n    \"\"\"Convert columns of dataframe to datetime format, as per given\"\"\"\n    if isinstance(columns, str):\n        columns = [columns]\n\n    for col in columns:\n        df[col] = pd.to_datetime(df.loc[:, col].values)\n\n    return df\n\n\ndef subsetted_by(df1: pd.DataFrame, df2: pd.DataFrame, matching_columns: Union[List[str], str]) -> pd.DataFrame:\n    \"\"\"\n    Subset df1 based on matching_columns, according to values existing in df2.\n\n    Returns: pd.DataFrame subset of df1 for which rows are a subset of df2\n    \"\"\"\n\n    return df1.iloc[_rows_are_in(df1, df2, matching_columns), :]\n\n\ndef endpoint_target_ids(df: pd.DataFrame, identifier: str, time_col: str = \"intime\", mode: str = \"max\") -> pd.DataFrame:\n    \"\"\"\n    Given identifier target (\"id\"), compute the endpoint associated with time column.\n\n    Returns: pd.DataFrame with ids and associated endpoint information.\n    \"\"\"\n    last_ids = df.groupby(identifier, as_index=False).progress_apply(\n        lambda x: _compute_last_target_id(x, time_col=time_col, mode=mode))\n\n    return last_ids.reset_index(drop=True)\n\n\ndef compute_second_transfer(df: pd.DataFrame, identifier: str, time_col: str, target_cols: pd.Index) -> pd.DataFrame:\n    \"\"\"\n    Given transfer data represented by unique identifier (\"id\"), compute the second transfer of the admission.\n    Second Transfer defined as second present intime in the date (if multiple, this is flagged). If there are\n    no transfers after, then return np.nan. target_cols is the target information columns.\n\n    This function checks the second transfer intime is after outtime of first transfer record.\n\n    Returns: pd.DataFrame with id and associated second transfer information (intime/outtime, unit, etc...)\n    \"\"\"\n    second_transfer_info = df.groupby(identifier, as_index=False).progress_apply(\n        lambda x: _compute_second_transfer_info(x, time_col, target_cols))\n\n    return second_transfer_info.reset_index(drop=True)\n\n\ndef _has_many_nas(df: pd.DataFrame, targets: Union[List[str], str], min_count: int, min_frac: float) -> bool:\n    \"\"\"\n    For a given admission/stay with corresponding vital sign information, return boolean indicating whether low\n    missingness conditions are satisfied. These are:\n    a) At least min_count observations.\n    b) Proportion of missing values smaller than min_frac for ALL targets.\n\n    returns: boolean indicating admission should be kept.\n    \"\"\"\n    if isinstance(targets, str):\n        targets = [targets]\n\n    has_minimum_counts = df.shape[0] > min_count\n    has_less_NA_than_frac = df[targets].isna().sum() <= min_frac * df.shape[0]\n\n    return has_minimum_counts and has_less_NA_than_frac.all()\n\n\ndef remove_adms_high_missingness(df: pd.DataFrame, targets: Union[List[str], str],\n                                 identifier: str, min_count: int, min_frac: float) -> pd.DataFrame:\n    \"\"\"\n    Given vital sign data, remove admissions with too little information. This is defined as either:\n    a) Number of observations smaller than allowed min_count.\n    b) Proportion of missing values in ANY of the targets is higher than min_frac.\n\n    Returns: pd.DataFrame - Vital sign data of the same type, except only admissions with enough information are kept.\n    \"\"\"\n    output = df.groupby(identifier, as_index=False).filter(\n        lambda x: _has_many_nas(x, targets, min_count, min_frac))\n\n    return output.reset_index(drop=True)\n\n\ndef _resample_adm(df: pd.DataFrame, rule: str, time_id: str,\n                  time_vars: Union[List[str], str], static_vars: Union[List[str], str]) -> pd.DataFrame:\n    \"\"\"\n    For a particular stay with vital sign data as per df, resample trajectory data (subsetted to time_vars),\n    according to index given by time_to_end and as defined by rule. It is important that time_to_end decreases\n    throughout admissions and hits 0 at the end - this is for resampling purposes.\n\n    Params:\n    df: pd.Dataframe, containing trajectory and static data for each admission.\n    rule: str, indicates the resampling rule (to be fed to pd.DataFrame.resample())\n\n    static_vars is a list of relevant identifier information\n\n    returns: Resampled admission data. Furthermore, two more info columns are indicated (chartmax and chartmin).\n    \"\"\"\n    if isinstance(time_vars, str):\n        time_vars = [time_vars]\n\n    if isinstance(static_vars, str):\n        static_vars = [static_vars]\n\n    # Add fake observation (with missing values) so that resampling starts at end of admission\n    df_inter = df[time_vars + [\"time_to_end\"]]\n    df_inter = df_inter.append(pd.Series(data=[np.nan] * len(time_vars) + [dt.timedelta(seconds=0)],\n                                         index=df_inter.columns), ignore_index=True)\n\n    # resample on time_to_end axis\n    output = df_inter.sort_values(by=\"time_to_end\", ascending=False).resample(\n        on=\"time_to_end\",\n        rule=rule, closed=\"left\", label=\"left\").mean()\n\n    # Compute static ids manually and add information about max and min time id values\n    output[static_vars] = df[static_vars].iloc[0, :].values\n    output[time_id + \"_min\"] = df[time_id].min()\n    output[time_id + \"_max\"] = df[time_id].max()\n\n    # Reset index to obtain resampled values\n    output.index.name = f\"sampled_time_to_end({rule})\"\n    output.reset_index(drop=False, inplace=True)\n\n    return output\n\n\ndef compute_time_to_end(df: pd.DataFrame, id_key: str, time_id: str, end_col: str):\n    \"\"\"\n    Compute time to end of admission for a given observation associated with a particular admission id.\n\n    df: pd.DataFrame with trajectory information.\n    id_key: str - column of df representing the unique id admission identifier.\n    time_id: str - column of df indicating time observations was taken.\n    end_col: str - column of df indicating, for each observation, the end time of the corresponding admission.\n\n    returns: sorted pd.DataFrame with an extra column indicating time to end of admission. This will be used for\n    resampling.\n    \"\"\"\n    df_inter = df.copy()\n    df_inter[\"time_to_end\"] = df_inter[end_col] - df_inter[time_id]\n    df_inter.sort_values(by=[id_key, \"time_to_end\"], ascending=[True, False], inplace=True)\n\n    return df_inter\n\n\ndef conversion_to_block(df: pd.DataFrame, id_key: str, rule: str,\n                        time_vars: Union[List[str], str], static_vars: Union[List[str], str]) -> pd.DataFrame:\n    \"\"\"\n    Given trajectory data over multiple admissions (as specified by id), resample each admission according to time\n    until the end of the admission. Resampling according to rule and apply to_time_vars.\n\n    df: pd.DataFrame containing trajectory and static data.\n    id_key: str, unique identifier per admission\n    rule: str, indicates resampling rule (to be fed to pd.DataFrame.resample())\n    time_vars: list of str, indicates columns of df to be resampled.\n    static_vars: list of str, indicates columns of df which are static, and therefore not resampled.\n\n    return: Dataframe with resampled vital sign data.\n    \"\"\"\n    if \"time_to_end\" not in df.columns:\n        raise ValueError(\"'time_to_end' not found in columns of dataframe. Run 'compute_time_to_end' function first.\")\n    assert df[id_key].is_monotonic and df.groupby(id_key).apply(\n        lambda x: x[\"time_to_end\"].is_monotonic_decreasing).all()\n\n    # Resample admission according to time_to_end\n    output = df.groupby(id_key).progress_apply(lambda x: _resample_adm(x, rule, \"time_to_end\", time_vars, static_vars))\n\n    return output.reset_index(drop=True)\n\n\ndef convert_to_timedelta(df: pd.DataFrame, *args) -> pd.DataFrame:\n    \"\"\"Convert all given cols of dataframe to timedelta.\"\"\"\n    output = df.copy()\n    for arg in args:\n        output[arg] = pd.to_timedelta(df.loc[:, arg])\n\n    return output\n\n\ndef _check_all_tables_exist(folder_path: str):\n    \"\"\"TO MOVE TO TEST\"\"\"\n    try:\n        assert os.path.exists(folder_path)\n    except Exception:\n        raise ValueError(\"Folder path does not exist - Input {}\".format(folder_path))\n\n\ndef select_death_icu_acute(df, admissions_df, timedt):\n    \"\"\"\n    Identify outcomes based on severity within the consequent 12 hours:\n    a) Death\n    b) Entry to ICU Careunit\n    c) Transfer to hospital ward\n    d) Discharge\n\n    Params:\n    - df - transfers dataframe corresponding to a particular admission.\n    - timedt - datetime timedelta indicating range window of prediction\n\n    Returns categorical encoding of the corresponding admission.\n    Else returns 0,0,0,0 if a mistake is found.\n    \"\"\"\n    # Check admission contains only one such row\n    assert admissions_df.hadm_id.eq(df.name).sum() <= 1\n\n    # Identify Last observed vitals for corresponding admission\n    hadm_information = admissions_df.query(\"hadm_id==@df.name\").iloc[0, :]\n    window_start_point = hadm_information.loc[\"outtime\"]\n\n    # First check if death exists\n    hadm_information = admissions_df.query(\"hadm_id==@df.name\")\n    if not hadm_information.empty and not hadm_information.dod.isna().all():\n        time_of_death = hadm_information.dod.min()\n        time_from_start_point = (time_of_death - window_start_point)\n\n        # try:\n        #     assert time_from_vitals >= dt.timedelta(seconds=0)\n        #\n        # except AssertionError:\n        #     return pd.Series(data=[0, 0, 0, 0, time_of_death], index=[\"De\", \"I\", \"W\", \"Di\", \"time\"])\n\n        # Check death within time window\n        if time_from_start_point < timedt:\n            return pd.Series(data=[1, 0, 0, 0, time_of_death], index=[\"De\", \"I\", \"W\", \"Di\", \"time\"])\n\n    # Otherwise, consider other transfers\n    transfers_within_window = df[df[\"intime\"].between(window_start_point, window_start_point + timedt)]\n\n    # Consider icu transfers within window\n    icu_cond1 = transfers_within_window.careunit.str.contains(\"(?i)ICU\", na=False)  # regex ignore lowercase\n    icu_cond2 = transfers_within_window.careunit.str.contains(\"(?i)Neuro Stepdown\", na=False)\n    has_icus = (icu_cond1 | icu_cond2)\n\n    if has_icus.sum() > 0:\n        icu_transfers = transfers_within_window[has_icus]\n        return pd.Series(data=[0, 1, 0, 0, icu_transfers.intime.min()],\n                         index=[\"De\", \"I\", \"W\", \"Di\", \"time\"])\n\n    # Check to see if discharge has taken\n    discharges = transfers_within_window.eventtype.str.contains(\"discharge\", na=False)\n    if discharges.sum() > 0:\n        return pd.Series(data=[0, 0, 0, 1, transfers_within_window[discharges].intime.min()],\n                         index=[\"De\", \"I\", \"W\", \"Di\", \"time\"]\n                         )\n    else:\n        return pd.Series(data=[0, 0, 1, 0, transfers_within_window.intime.min()],\n                         index=[\"De\", \"I\", \"W\", \"Di\", \"time\"]\n                         )\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T23:47:39.388113Z","iopub.execute_input":"2023-05-01T23:47:39.388715Z","iopub.status.idle":"2023-05-01T23:47:39.438801Z","shell.execute_reply.started":"2023-05-01T23:47:39.388654Z","shell.execute_reply":"2023-05-01T23:47:39.437934Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\n\n# ---------------------------------------------------------------------------------------\n\"Global variables for specific dataset information loading.\"\n\nMIMIC_PARSE_TIME_VARS = [\"intime\", \"outtime\", \"chartmax\"]\nMIMIC_PARSE_TD_VARS = [\n    \"sampled_time_to_end(1H)\", \"time_to_end\", \"time_to_end_min\", \"time_to_end_max\"]\nMIMIC_VITALS = [\"TEMP\", \"HR\", \"RR\", \"SPO2\", \"SBP\", \"DBP\"]\nMIMIC_STATIC = [\"age\", \"gender\", \"ESI\"]\nMIMIC_OUTCOME_NAMES = [\"De\", \"I\", \"W\", \"Di\"]\n\n# Identifiers for main ids.\nMAIN_ID_LIST = [\"subject_id\", \"hadm_id\", \"stay_id\", \"patient_id\", \"pat_id\"]\n\n# ----------------------------------------------------------------------------------------\n\n\nclass CustomDataset(Dataset):\n\n    def __init__(self, data_name=\"MIMIC\", target_window=12, feat_set='vit-sta', time_range=(0, 6), parameters=None):\n        if parameters is None:\n            self.data_name = data_name\n            self.target_window = target_window\n            self.feat_set = feat_set\n            self.time_range = time_range\n            self.id_col = None\n            self.time_col = None\n            self.needs_time_to_end_computation = False\n            self.min = None\n            self.max = None\n\n            # Load and process data\n            self.id_col, self.time_col, self.needs_time_to_end_computation = self.get_ids(\n                self.data_name)\n            self.x, self.y, self.mask, self.pat_time_ids, self.features, self.outcomes, self.x_subset, self.y_data = self.load_transform()\n        else:\n            self.x, self.y, self.mask, self.pat_time_ids, self.features, self.outcomes, self.x_subset, self.y_data, self.id_col, self.time_col, self.needs_time_to_end_computation, self.data_name, self.feat_set, self.time_range, self.target_window, self.min, self.max = parameters\n\n    def __len__(self):\n        return self.x.shape[0]\n\n    def __getitem__(self, idx):\n        # Extract data for given index\n        x = self.x[idx, :, :]\n        y = self.y[idx, :]\n        mask = self.mask[idx, :, :]\n        pat_time_ids = self.pat_time_ids[idx, :, :]\n        features = self.features\n        outcomes = self.outcomes\n        x_subset = self.x_subset[idx, :]\n        y_data = self.y_data[idx, :]\n        id_col = self.id_col\n        time_col = self.time_col\n        needs_time_to_end_computation = self.needs_time_to_end_computation\n        data_name = self.data_name\n        feat_set = self.feat_set\n        time_range = self.time_range\n        target_window = self.target_window\n        min = self.min\n        max = self.max\n        return x, y, mask, pat_time_ids, features, outcomes, x_subset, y_data, id_col, time_col, needs_time_to_end_computation, data_name, feat_set, time_range, target_window, min, max\n\n    def get_subset(self, idx):\n        return CustomDataset(parameters=self[idx])\n\n    def load_transform(self):\n        \"\"\"Load dataset and transform to input format\"\"\"\n\n        # Load data\n        data = self._load(self.data_name, window=self.target_window)\n\n        # Get data info\n        self.id_col, self.time_col, self.needs_time_to_end_computation = self.get_ids(\n            self.data_name)\n\n        # Add time to end and truncate if needed\n        # print(data[0].shape, '0')\n        x_inter = self._add_time_to_end(data[0])\n        # print(x_inter.shape, '1')\n        x_inter = self._truncate(x_inter)\n        # print(x_inter.shape, '2')\n        self._check_correct_time_conversion(x_inter)\n\n        # Subset to relevant features (keeps self.id_col and self.time_col still)\n        # print(x_inter.shape, '3')\n        x_subset, features = self.subset_to_features(x_inter)\n\n        # Convert to 3D array\n        # print(x_inter.shape, '4')\n        x_inter, pat_time_ids = self.convert_to_3darray(x_subset)\n        x_subset = x_subset.to_numpy().astype(np.float32)\n\n        # Normalise array\n        # print(x_inter.shape, '5')\n        x_inter = self.normalise(x_inter)\n\n        # Impute missing values\n        # print(x_inter.shape, '6')\n        x_out, mask = self.impute(x_inter)\n        # print(x_out.shape, '7')\n\n        # Do things to y\n        outcomes = self._get_outcome_names(self.data_name)\n        y_data = data[1][outcomes]\n        y_out = y_data.to_numpy().astype(\"float32\")\n        y_data = y_data.to_numpy().astype(\"float32\")\n\n        # Check data loaded correctly\n        self._check_input_format(x_out, y_out)\n\n        return x_out, y_out, mask, pat_time_ids, features, outcomes, x_subset, y_data\n\n    def _load(self, data_name, window=4):\n        \"\"\"Load Trajectory, Target data jointly given data folder.\"\"\"\n\n        # Make data folder\n        # data_fd = f\"/kaggle/input/mimic-processed/\"\n        data_fd = f\"/kaggle/input/mimic-processed/\"\n        try:\n            os.path.exists(data_fd)\n        except AssertionError:\n            print(data_fd)\n\n        if \"MIMIC\" in data_name:\n\n            # Load Data\n            X = pd.read_csv(data_fd + \"vitals_process.csv\",\n                            parse_dates=MIMIC_PARSE_TIME_VARS, header=0, index_col=0)\n            y = pd.read_csv(\n                data_fd + f\"outcomes_{window}h_process.csv\", index_col=0)\n            # for Kaggle:\n#             X = pd.read_csv(\"vitals_process.csv\", parse_dates=MIMIC_PARSE_TIME_VARS, header=0, index_col=0)\n#             y = pd.read_csv(f\"outcomes_{window}h_process.csv\", index_col=0)\n\n            # Convert columns to timedelta\n            X = convert_to_timedelta(X, *MIMIC_PARSE_TD_VARS)\n\n        elif \"SAMPLE\" in data_name:\n\n            # Load data\n            X = None\n            y = None\n\n        else:\n            raise ValueError(\n                f\"Data Name does not match available datasets. Input Folder provided {data_fd}\")\n        return X, y\n\n    def get_ids(self, data_name):\n        \"\"\"\n        Get input id information.\n\n        Params:\n        - data_folder: str, folder of dataset, or name of dataset.\n\n        Returns:\n            - Tuple of id col, time col and whether time to end needs computation.\n        \"\"\"\n\n        if \"MIMIC\" in data_name:\n            id_col, time_col, needs_time_to_end = \"hadm_id\", \"sampled_time_to_end(1H)\", False\n\n        elif \"SAMPLE\" in data_name:\n            id_col, time_col, needs_time_to_end = None, None, None\n\n        else:\n            raise ValueError(\n                f\"Data Name does not match available datasets. Input Folder provided {data_name}\")\n\n        return id_col, time_col, needs_time_to_end\n\n    def impute(self, X):\n        \"\"\"\n        Imputation of 3D array accordingly with time as dimension 1:\n        1st - forward value propagation,\n        2nd - backwards value propagation,\n        3rd - median value imputation.\n\n        Mask returned at the end, corresponding to original missing values.\n        \"\"\"\n        impute_step1 = self._numpy_forward_fill(X)\n        impute_step2 = self._numpy_backward_fill(impute_step1)\n        impute_step3 = self._median_fill(impute_step2)\n\n        # Compute mask\n        mask = np.isnan(X)\n\n        return impute_step3, mask\n\n    def convert_datetime_to_hour(self, series):\n        \"\"\"Convert pandas Series of datetime values to float Series with corresponding hour values\"\"\"\n        seconds_per_hour = 3600\n\n        return series.dt.total_seconds() / seconds_per_hour\n\n    def _get_features(self, key, data_name=\"MIMIC\"):\n        \"\"\"\n        Compute list of features to keep given key. Key can be one of:\n        - str, where the corresponding features are selected according to the fn below.\n        - list, where the corresponding features are the original list.\n        \"\"\"\n        if isinstance(key, list):\n            return key\n\n        elif isinstance(key, str):\n            if data_name == \"MIMIC\":\n                vitals = MIMIC_VITALS\n                static = MIMIC_STATIC\n                vars1, vars2 = None, None\n\n            elif data_name == \"SAMPLE\":\n                vitals, vars1, vars2, static = None, None, None, None\n\n            else:\n                raise ValueError(\n                    f\"Data Name does not match available datasets. Input provided {data_name}\")\n\n            # Add features given substrings of key. We initialise set in case of repetition (e.g. 'vars1-lab')\n            features = set([])\n            if \"vit\" in key.lower():\n                features.update(vitals)\n\n            if \"vars1\" in key.lower():\n                features.update(vars1)\n\n            if \"vars2\" in key.lower():\n                features.update(vars2)\n\n            if \"lab\" in key.lower():\n                features.update(vars1)\n                features.update(vars2)\n\n            if \"sta\" in key.lower():\n                features.update(static)\n\n            if \"all\" in key.lower():\n                features = self._get_features(\"vit-lab-sta\", data_name)\n\n            # sorted returns a list of features.\n            sorted_features = sorted(features)\n            print(\n                f\"\\n{data_name} data has been subsettted to the following features: \\n {sorted_features}.\")\n\n            return sorted_features\n\n        else:\n            raise TypeError(\n                f\"Argument key must be one of type str or list, type {type(key)} was given.\")\n\n    def _numpy_forward_fill(self, array):\n        \"\"\"Forward Fill a numpy array. Time index is axis = 1.\"\"\"\n        array_mask = np.isnan(array)\n        array_out = np.copy(array)\n\n        # Add time indices where not masked, and propagate forward\n        inter_array = np.where(~ array_mask, np.arange(\n            array_mask.shape[1]).reshape(1, -1, 1), 0)\n        np.maximum.accumulate(inter_array, axis=1,\n                              out=inter_array)  # For each (n, t, d) missing value, get the previously accessible mask value\n\n        # Index matching for output. For n, d sample as previously, use inter_array for previous time id\n        array_out = array_out[np.arange(array_out.shape[0])[:, None, None],\n                              inter_array,\n                              np.arange(array_out.shape[-1])[None, None, :]]\n\n        return array_out\n\n    def _numpy_backward_fill(self, array):\n        \"\"\"Backward Fill a numpy array. Time index is axis = 1\"\"\"\n        array_mask = np.isnan(array)\n        array_out = np.copy(array)\n\n        # Add time indices where not masked, and propagate backward\n        inter_array = np.where(~ array_mask, np.arange(\n            array_mask.shape[1]).reshape(1, -1, 1), array_mask.shape[1] - 1)\n        inter_array = np.minimum.accumulate(\n            inter_array[:, ::-1], axis=1)[:, ::-1]\n        array_out = array_out[np.arange(array_out.shape[0])[:, None, None],\n                              inter_array,\n                              np.arange(array_out.shape[-1])[None, None, :]]\n\n        return array_out\n\n    def _median_fill(self, array):\n        \"\"\"Median fill a numpy array. Time index is axis = 1\"\"\"\n        array_mask = np.isnan(array)\n        array_out = np.copy(array)\n\n        # Compute median and impute\n        array_med = np.nanmedian(np.nanmedian(\n            array, axis=0, keepdims=True), axis=1, keepdims=True)\n        array_out = np.where(array_mask, array_med, array_out)\n\n        return array_out\n\n    def _get_outcome_names(self, data_name):\n        \"\"\"Return the corresponding outcome columns given dataset name.\"\"\"\n        if data_name == \"MIMIC\":\n            return MIMIC_OUTCOME_NAMES\n\n        elif data_name == \"SAMPLE\":\n            return None\n\n    def _check_input_format(self, X, y):\n        \"\"\"Check conditions to confirm model input.\"\"\"\n\n        try:\n            # Length and shape conditions\n            # print(X.shape, y.shape)\n            cond1 = X.shape[0] == y.shape[0]\n            cond2 = len(X.shape) == 3\n            cond3 = len(y.shape) == 2\n\n            # Check non-missing values\n            cond4 = np.sum(np.isnan(X)) + np.sum(np.isnan(y)) == 0\n\n            # Check y output is one hot encoded\n            cond5 = np.all(np.sum(y, axis=1) == 1)\n\n            assert cond1\n            assert cond2\n            assert cond3\n            assert cond4\n            assert cond5\n\n        except Exception as e:\n            print(e)\n            raise AssertionError(\"One of the check conditions has failed.\")\n\n    def _subset_to_balanced(X, y, mask, ids):\n        \"\"\"Subset samples so dataset is more well sampled.\"\"\"\n        class_numbers = np.sum(y, axis=0)\n        largest_class, target_num_samples = np.argmax(\n            class_numbers), np.sort(class_numbers)[-2]\n        print(\"\\nSubsetting class {} from {} to {} samples.\".format(largest_class, class_numbers[largest_class],\n                                                                    target_num_samples))\n\n        # Select random\n        largest_class_ids = np.arange(y.shape[0])[y[:, largest_class] == 1]\n        class_ids_samples = np.random.choice(\n            largest_class_ids, size=target_num_samples, replace=False)\n        ids_to_remove_ = np.setdiff1d(largest_class_ids, class_ids_samples)\n\n        # Remove relevant ids\n        X_out = np.delete(X, ids_to_remove_, axis=0)\n        y_out = np.delete(y, ids_to_remove_, axis=0)\n        mask_out = np.delete(mask, ids_to_remove_, axis=0)\n        ids_out = np.delete(ids, ids_to_remove_, axis=0)\n\n        return X_out, y_out, mask_out, ids_out\n\n    def _add_time_to_end(self, X):\n        \"\"\"Add new column to dataframe - this computes time to end of grouped observations, if needed.\"\"\"\n        x_inter = X.copy(deep=True)\n\n        # if time to end has not been computed\n        if self.needs_time_to_end_computation is True:\n\n            # Compute datetime values for time until end of group of observations\n            times = X.groupby(self.id_col).apply(\n                lambda x: x.loc[:, self.time_col].max() - x.loc[:, self.time_col])\n\n            # add column to dataframe after converting to hourly times.\n            x_inter[\"time_to_end\"] = self.convert_datetime_to_hour(\n                times).values\n\n        else:\n            x_inter[\"time_to_end\"] = x_inter[self.time_col].values\n            x_inter[\"time_to_end\"] = self.convert_datetime_to_hour(\n                x_inter.loc[:, \"time_to_end\"])\n\n        # Sort data\n        self.time_col = \"time_to_end\"\n        x_out = x_inter.sort_values(\n            by=[self.id_col, \"time_to_end\"], ascending=[True, False])\n\n        return x_out\n\n    def _truncate(self, X):\n        \"\"\"Truncate dataset on time to end column according to self.time_range.\"\"\"\n        try:\n            min_time, max_time = self.time_range\n            # print(self.time_range)\n            return X[X['time_to_end'].between(min_time, max_time, inclusive=\"left\")]\n\n        except Exception:\n            raise ValueError(\n                f\"Could not truncate to {self.time_range} time range successfully\")\n\n    def _check_correct_time_conversion(self, X):\n        \"\"\"Check addition and truncation of time index worked accordingly.\"\"\"\n\n        cond1 = X[self.id_col].is_monotonic_increasing\n        cond2 = X.groupby(self.id_col).apply(\n            lambda x: x[\"time_to_end\"].is_monotonic_decreasing).all()\n\n        min_time, max_time = self.time_range\n        cond3 = X[\"time_to_end\"].between(\n            min_time, max_time, inclusive='left').all()\n\n        assert cond1 is True\n        assert cond2 == True\n        assert cond3 == True\n\n    def subset_to_features(self, X):\n        \"\"\"Subset only to variables which were selected\"\"\"\n        features = [self.id_col, \"time_to_end\"] + \\\n            self._get_features(self.feat_set, self.data_name)\n\n        return X[features], features\n\n    def convert_to_3darray(self, X):\n        \"\"\"Convert a pandas dataframe to 3D numpy array of shape (num_samples, num_timestamps, num_variables).\"\"\"\n\n        # Obtain relevant shape sizes\n        max_time_length = X.groupby(self.id_col).count()[\"time_to_end\"].max()\n        num_ids = X[self.id_col].nunique()\n\n        # Other basic definitions\n        feats = [col for col in X.columns if col not in [\n            self.id_col, \"time_to_end\"]]\n        list_ids = X[self.id_col].unique()\n\n        # Initialise output array and id-time array\n        out_array = np.empty(shape=(num_ids, max_time_length, len(feats)))\n        out_array[:] = np.nan\n\n        # Make a parallel array indicating id and corresponding time\n        id_times_array = np.empty(shape=(num_ids, max_time_length, 2))\n\n        # Set ids in this newly generated array\n        id_times_array[:, :, 0] = np.repeat(np.expand_dims(\n            list_ids, axis=-1), repeats=max_time_length, axis=-1)\n\n        # Iterate through ids\n        for id_ in tqdm(list_ids):\n            # Subset data to where matches respective id\n            index_ = np.where(list_ids == id_)[0]\n            x_id = X[X[self.id_col] == id_]\n\n            # Compute negative differences instead of keeping the original times.\n            x_id_copy = x_id.copy()\n            x_id_copy[\"time_to_end\"] = - x_id[\"time_to_end\"].diff().values\n\n            # Update target output array and time information array\n            out_array[index_, :x_id_copy.shape[0], :] = x_id_copy[feats].values\n            id_times_array[index_, :x_id_copy.shape[0],\n                           1] = x_id[\"time_to_end\"].values\n\n        return out_array.astype(\"float32\"), id_times_array.astype(\"float32\")\n\n    def normalise(self, X):\n        \"\"\"Given 3D array, normalise according to min-max method.\"\"\"\n        self.min = np.nanmin(X, axis=0, keepdims=True)\n        self.max = np.nanmax(X, axis=0, keepdims=True)\n\n        return np.divide(X - self.min, self.max - self.min)\n\n    def apply_normalisation(self, X):\n        \"\"\"Apply normalisation with current parameters to another dataset.\"\"\"\n        if self.min is None or self.max is None:\n            raise ValueError(\n                f\"Attributes min and/or max are not yet computed. Run 'normalise' method instead.\")\n\n        else:\n            return np.divide(X - self.min, self.max - self.min)\n\n\n# Custom Dataloader\ndef collate_fn(data):\n    x, y, mask, pat_time_ids, features, outcomes, x_subset, y_data, id_col, time_col, needs_time_to_end_computation, data_name, feat_set, time_range, target_window, min, max = zip(\n        *data)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    data_config = {\"data_name\": data_name, \"feat_set\": feat_set,\n                   \"time_range (h)\": time_range, \"target_window\": target_window}\n    data_properties = {\"feats\": features, \"id_col\": id_col, \"time_col\": time_col,\n                       \"norm_min\": min, \"norm_max\": max, \"outc_names\": outcomes}\n\n    x = torch.tensor(np.array(x))\n    y = torch.tensor(np.array(y))\n    x = x.to(device)\n    y = y.to(device)\n    # mask = torch.tensor(mask)\n    # pat_time_ids = torch.tensor(pat_time_ids)\n    # x_subset = torch.tensor(x_subset)\n    # y_data = torch.tensor(y_data)\n\n    return x, y\n\n\ndef load_data(train_dataset, val_dataset, test_dataset):\n    \"\"\"\n    Return a DataLoader instance basing on a Dataset instance, with batch_size specified.\n    set shuffle=???\n    \"\"\"\n\n    batch_size = 64\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n    return train_loader, val_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2023-05-01T23:52:10.375790Z","iopub.execute_input":"2023-05-01T23:52:10.376153Z","iopub.status.idle":"2023-05-01T23:52:10.428718Z","shell.execute_reply.started":"2023-05-01T23:52:10.376124Z","shell.execute_reply":"2023-05-01T23:52:10.427832Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FeatTimeAttention(nn.Module):\n    def __init__(self, latent_dim, input_shape):\n        super().__init__()\n\n        self.device = torch.device(\n            'cuda' if torch.cuda.is_available() else 'cpu')\n\n        self.latent_dim = latent_dim\n        T, D_f = input_shape\n        # Define Kernel and Bias for Feature Projection\n        self.kernel = torch.zeros(\n            (1, 1, D_f, self.latent_dim), requires_grad=True).to(self.device)\n        nn.init.xavier_uniform_(self.kernel)\n        self.bias = torch.zeros(\n            (1, 1, D_f, self.latent_dim), requires_grad=True).to(self.device)\n        nn.init.uniform_(self.bias)\n\n        # Define Time aggregation weights for averaging over time.\n        self.unnorm_beta = torch.zeros((1, T, 1), requires_grad=True)\n        nn.init.uniform_(self.unnorm_beta)\n\n    def forward(self, x, latent):\n        o_hat, _ = self.generate_latent_approx(x, latent)\n        weights = self.calc_weights(self.unnorm_beta)\n        # print(o_hat.shape, weights.shape)\n        return torch.sum(torch.mul(o_hat.to(self.device), weights.to(self.device)), dim=1)\n\n    def generate_latent_approx(self, x, latent):\n        features = torch.mul(x.unsqueeze(-1), self.kernel) + self.bias\n        features = F.relu(features)\n\n        # calculate the score\n        X_T, X = features, features.transpose(2, 3)\n        # print(X_T.shape, X.shape)\n        X_T_X_inv = torch.inverse(torch.matmul(X_T, X))\n        # print(X_T.shape, latent.unsqueeze(-1).shape)\n        X_T_y = torch.matmul(X_T, latent.unsqueeze(-1))\n\n        score_hat = torch.matmul(X_T_X_inv, X_T_y)\n        scores = torch.squeeze(score_hat)\n\n        # print(scores.unsqueeze(-1).shape, features.shape)\n        o_hat = torch.sum(torch.mul(scores.unsqueeze(-1), features), dim=2)\n\n        return o_hat, scores\n\n    def calc_weights(self, x):\n        abs_x = torch.abs(x)\n        return abs_x / torch.sum(abs_x, dim=1)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_shape, attention_hidden_dim, latent_dim, dropout):\n        super().__init__()\n        self.lstm1 = nn.LSTM(input_size=input_shape[1],\n                             hidden_size=attention_hidden_dim,\n                             num_layers=2,\n                             dropout=dropout,\n                             batch_first=True)\n        self.lstm2 = nn.LSTM(input_size=attention_hidden_dim,\n                             hidden_size=latent_dim,\n                             num_layers=1,\n                             batch_first=True)\n        self.attention = FeatTimeAttention(latent_dim, input_shape)\n\n    def forward(self, x):\n        latent_rep, _ = self.lstm1(x)\n        latent_rep, _ = self.lstm2(latent_rep)\n        output = self.attention(x, latent_rep)\n        return output\n\n\nclass Identifier(nn.Module):\n    def __init__(self, input_dim, mlp_hidden_dim, dropout, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, mlp_hidden_dim)\n        self.sigmoid1 = nn.Sigmoid()\n\n        self.fc2 = nn.Linear(mlp_hidden_dim, mlp_hidden_dim)\n        self.sigmoid2 = nn.Sigmoid()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.fc4 = nn.Linear(mlp_hidden_dim, output_dim)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.sigmoid1(x)\n\n        x = self.fc2(x)\n        x = self.sigmoid2(x)\n        x = self.dropout1(x)\n\n        x = self.fc4(x)\n        x = self.softmax(x)\n        return x\n\n\nclass Predictor(nn.Module):\n    def __init__(self, input_dim, mlp_hidden_dim, dropout, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, mlp_hidden_dim)\n        self.sigmoid1 = nn.Sigmoid()\n\n        self.fc2 = nn.Linear(mlp_hidden_dim, mlp_hidden_dim)\n        self.sigmoid2 = nn.Sigmoid()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.fc3 = nn.Linear(mlp_hidden_dim, mlp_hidden_dim)\n        self.sigmoid3 = nn.Sigmoid()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.fc4 = nn.Linear(mlp_hidden_dim, output_dim)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.sigmoid1(x)\n\n        x = self.fc2(x)\n        x = self.sigmoid2(x)\n        x = self.dropout1(x)\n\n        x = self.fc3(x)\n        x = self.sigmoid3(x)\n        x = self.dropout2(x)\n\n        x = self.fc4(x)\n        x = self.softmax(x)\n        return x\n\n\nclass MyLRScheduler():\n    def __init__(self, optimizer, patience, min_lr, factor):\n        self.optimizer = optimizer\n        self.patience = patience\n        self.min_lr = min_lr\n        self.factor = factor\n        self.wait = 0\n        self.best_loss = float('inf')\n\n    def step(self, val_loss):\n        if val_loss < self.best_loss:\n            self.best_loss = val_loss\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.wait = 0\n                for param_group in self.optimizer.param_groups:\n                    old_lr = param_group['lr']\n                    new_lr = max(old_lr * self.factor, self.min_lr)\n                    param_group['lr'] = new_lr\n\n\ndef calc_l1_l2_loss(part=None, layers=None):\n    para = []\n    if part:\n        for parameter in part.parameters():\n            para.append(parameter.view(-1))\n        parameters = torch.cat(para)\n    if layers:\n        for layer in layers:\n            para.extend(layer.parameters())\n        parameters = torch.cat([p.view(-1) for p in para])\n    return 1e-30 * torch.abs(parameters).sum() + 1e-30 * torch.square(parameters).sum()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T23:47:39.524215Z","iopub.execute_input":"2023-05-01T23:47:39.524487Z","iopub.status.idle":"2023-05-01T23:47:39.548261Z","shell.execute_reply.started":"2023-05-01T23:47:39.524463Z","shell.execute_reply":"2023-05-01T23:47:39.547348Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\n\n\ndef np_log(x):\n    return np.log(x + 1e-8)\n\n\ndef torch_log(x):\n    return torch.log(x + 1e-8)\n\n\ndef calc_pred_loss(y_true, y_pred, weights=None):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    if weights is None:\n        weights = torch.ones(y_true.shape) / y_true.shape[-1]\n    return - torch.mean(torch.sum(weights.to(device) * y_true.to(device) * torch_log(y_pred).to(device), axis=-1))\n\n\ndef calc_dist_loss(probs):\n    avg_prob = torch.mean(probs, dim=-1)\n    return torch.sum(avg_prob * torch.log(avg_prob))\n\n\ndef calc_clus_loss(clusters):\n    pairewise_loss = - \\\n        torch.sum((clusters.unsqueeze(1) - clusters.unsqueeze(0)) ** 2, dim=-1)\n    loss = torch.sum(pairewise_loss)\n\n    K = clusters.shape[0]\n    return (loss / (K * (K-1) / 2)).float()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T23:47:39.549564Z","iopub.execute_input":"2023-05-01T23:47:39.550198Z","iopub.status.idle":"2023-05-01T23:47:39.561601Z","shell.execute_reply.started":"2023-05-01T23:47:39.550166Z","shell.execute_reply":"2023-05-01T23:47:39.560717Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport numpy as np\nfrom tqdm import trange\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\n\nSEED = 12345\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ndef class_weight(y):\n    class_numbers = torch.sum(y, dim=0)\n\n    # Check no class is missing\n    if not torch.all(class_numbers > 0):\n        class_numbers += 1\n    inv_class_num = 1 / class_numbers\n    return inv_class_num / torch.sum(inv_class_num)\n\n\nclass CamelotModel(nn.Module):\n    def __init__(self, input_shape, num_clusters=10, latent_dim=128, seed=SEED, output_dim=4,\n                 alpha=0.01, beta=0.001, regularization=(0.01, 0.01), dropout=0.0,\n                 cluster_rep_lr=0.001, weighted_loss=True, attention_hidden_dim=16,\n                 mlp_hidden_dim=30):\n\n        super().__init__()\n        self.seed = seed\n\n        self.input_shape = input_shape\n        self.num_clusters = num_clusters\n        self.latent_dim = latent_dim\n        self.output_dim = output_dim\n        self.alpha = alpha\n        self.beta = beta\n        self.regularization = regularization\n        self.dropout = dropout\n        self.cluster_rep_lr = cluster_rep_lr\n        self.weighted_loss = weighted_loss\n        self.attention_hidden_dim = attention_hidden_dim\n        self.mlp_hidden_dim = mlp_hidden_dim\n\n        # three newtorks\n        self.Encoder = Encoder(\n            self.input_shape, self.attention_hidden_dim, self.latent_dim, self.dropout)\n        self.Identifier = Identifier(\n            self.latent_dim, self.mlp_hidden_dim, self.dropout, self.num_clusters)\n        self.Predictor = Predictor(\n            self.latent_dim, self.mlp_hidden_dim, self.dropout, self.output_dim)\n\n        # Cluster Representation params\n        self.cluster_rep_set = torch.zeros(\n            size=[self.num_clusters, self.latent_dim], dtype=torch.float32, requires_grad=True)\n\n        self.loss_weights = None\n\n    def forward(self, x):\n        z = self.Encoder(x)\n        probs = self.Identifier(z)\n        samples = self.get_sample(probs)\n        representations = self.get_representations(samples)\n        return self.Predictor(representations)\n\n    def forward_pass(self, x):\n        z = self.Encoder(x)\n        probs = self.Identifier(z)\n        # print(probs.shape)\n        # samples = self.get_sample(probs)\n        # # print(samples.shape)\n        # representations = self.get_representations(samples)\n        # # print(representations.shape)\n        # return self.Predictor(representations), probs\n        clus_phens = self.Predictor(self.cluster_rep_set.to(device))\n        y_pred = torch.matmul(probs, clus_phens)\n\n        return y_pred, probs\n\n    def get_sample(self, probs):\n        logits = - torch.log(probs.reshape(-1, self.num_clusters))\n        samples = torch.multinomial(logits, num_samples=1)\n        return samples.squeeze()\n\n    def get_representations(self, samples):\n        mask = F.one_hot(samples, num_classes=self.num_clusters).to(\n            torch.float32)\n        return torch.matmul(mask.to(device), self.cluster_rep_set.to(device))\n\n    def calc_pis(self, X):\n        return self.Identifier(self.Encoder(X)).numpy()\n\n    def get_cluster_reps(self):\n        return self.cluster_rep_set.numpy()\n\n    def assignment(self, X):\n        pi = self.Identifier(self.Encoder(X)).numpy()\n        return torch.argmax(pi, dim=1)\n\n    def compute_cluster_phenotypes(self):\n        return self.Predictor(self.cluster_rep_set).numpy()\n\n    # def compute_unnorm_attention_weights(self, inputs):\n    #     # no idea\n    #     return self.Encoder.compute_unnorm_scores(inputs, cluster_reps=self.cluster_rep_set)\n\n    # def compute_norm_attention_weights(self, inputs):\n    #     # no idea\n    #     return self.Encoder.compute_norm_scores(inputs, cluster_reps=self.cluster_rep_set)\n\n    def initialize(self, train_data, val_data):\n        x_train, y_train = train_data\n        x_val, y_val = val_data\n        self.loss_weights = class_weight(y_train)\n\n        # initialize encoder\n        self.initialize_encoder(x_train, y_train, x_val, y_val)\n\n        # initialize cluster\n        clus_train, clus_val = self.initialize_cluster(x_train, x_val)\n        self.clus_train = clus_train\n        self.x_train = x_train\n\n        # initialize identifier\n        self.initialize_identifier(x_train, clus_train, x_val, clus_val)\n\n    def initialize_encoder(self, x_train, y_train, x_val, y_val, epochs=100, batch_size=64):\n        temp = DataLoader(\n            dataset=TensorDataset(x_train, y_train),\n            shuffle=True,\n            batch_size=batch_size\n        )\n\n        iden_loss = torch.full((epochs,), float('nan'))\n        initialize_optim = torch.optim.Adam(\n            self.Encoder.parameters(), lr=0.001)\n\n        for i in trange(epochs):\n            epoch_loss = 0\n            for _, (x_batch, y_batch) in enumerate(temp):\n                initialize_optim.zero_grad()\n\n                z = self.Encoder(x_batch)\n                y_pred = self.Predictor(z)\n                loss = calc_pred_loss(\n                    y_batch, y_pred, self.loss_weights) + calc_l1_l2_loss(part=self.Encoder)\n\n                loss.backward()\n                initialize_optim.step()\n\n                epoch_loss += loss.item()\n\n            with torch.no_grad():\n                z = self.Encoder(x_val)\n                y_pred_val = self.Predictor(z)\n                loss_val = calc_pred_loss(y_val, y_pred_val, self.loss_weights)\n\n            iden_loss[i] = loss_val.item()\n            if torch.le(iden_loss[-50:], loss_val.item() + 0.001).any():\n                break\n\n        print('Encoder initialization done!')\n\n    def initialize_cluster(self, x_train, x_val):\n        z = self.Encoder(x_train).cpu().detach().numpy()\n        kmeans = KMeans(self.num_clusters, random_state=self.seed, n_init='auto')\n        kmeans.fit(z)\n        print('Kmeans initialization done!')\n\n        self.cluster_rep_set = torch.tensor(\n            kmeans.cluster_centers_, dtype=torch.float32, requires_grad=True)\n        train_cluster = torch.eye(self.num_clusters)[\n            kmeans.predict(z)]\n        val_cluster = torch.eye(self.num_clusters)[kmeans.predict(\n            self.Encoder(x_val).cpu().detach().numpy())]\n\n        print('Cluster initialization done!')\n        return train_cluster, val_cluster\n\n    def initialize_identifier(self, x_train, clus_train, x_val, clus_val, epochs=100, batch_size=64):\n        temp = DataLoader(\n            dataset=TensorDataset(x_train, clus_train),\n            shuffle=True,\n            batch_size=batch_size\n        )\n\n        iden_loss = torch.full((epochs,), float('nan'))\n        initialize_optim = torch.optim.Adam(\n            self.Identifier.parameters(), lr=0.001)\n\n        for i in trange(epochs):\n            epoch_loss = 0\n            for step_, (x_batch, clus_batch) in enumerate(temp):\n                initialize_optim.zero_grad()\n\n                clus_pred = self.Identifier(self.Encoder(x_batch))\n                loss = calc_pred_loss(clus_batch, clus_pred) + \\\n                    calc_l1_l2_loss(layers=[self.Identifier.fc2])\n\n                loss.backward()\n                initialize_optim.step()\n\n                epoch_loss += loss.item()\n\n            with torch.no_grad():\n                clus_pred_val = self.Identifier(self.Encoder(x_val))\n                loss_val = calc_pred_loss(clus_val, clus_pred_val)\n\n            iden_loss[i] = loss_val.item()\n            if torch.le(iden_loss[-50:], loss_val.item() + 0.001).any():\n                break\n\n        print('Identifier initialization done!')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T23:48:04.062117Z","iopub.execute_input":"2023-05-01T23:48:04.062533Z","iopub.status.idle":"2023-05-01T23:48:04.130867Z","shell.execute_reply.started":"2023-05-01T23:48:04.062500Z","shell.execute_reply":"2023-05-01T23:48:04.129912Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"results = np.zeros((len(seeds), 4))\nfor index, SEED in enumerate(seeds):\n    torch.random.manual_seed(SEED)\n    np.random.seed(SEED)\n    random.seed(SEED)\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    dataset = CustomDataset(time_range=(0, 10))\n\n    # Stratified Sampling for train and val\n    train_idx, test_idx = train_test_split(np.arange(len(dataset)),\n                                                test_size=0.4,\n                                                random_state=SEED,\n                                                shuffle=True,\n                                                stratify=np.argmax(dataset.y,axis=-1))\n\n    # Subset dataset for train and val\n    train_val_dataset = dataset.get_subset(train_idx)\n    test_dataset = dataset.get_subset(test_idx)\n\n    train_idx,  val_idx = train_test_split(np.arange(len(train_val_dataset)),\n                                                test_size=0.4,\n                                                random_state=SEED,\n                                                shuffle=True,\n                                                stratify=np.argmax(train_val_dataset.y,axis=-1))\n\n    train_dataset = train_val_dataset.get_subset(train_idx)\n    val_dataset = train_val_dataset.get_subset(val_idx)\n\n    train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset)\n\n    model = CamelotModel(input_shape=(train_dataset.x.shape[1], train_dataset.x.shape[2]), seed=SEED, num_clusters=10, latent_dim=64)\n    model = model.to(device)\n\n    train_x = torch.tensor(train_dataset.x).to(device)\n    train_y = torch.tensor(train_dataset.y).to(device)\n    val_x = torch.tensor(val_dataset.x).to(device)\n    val_y = torch.tensor(val_dataset.y).to(device)\n\n    model.initialize((train_x, train_y), (val_x, val_y))\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    cluster_optim = torch.optim.Adam([model.cluster_rep_set], lr=0.001)\n\n    lr_scheduler = MyLRScheduler(optimizer, patience=15, min_lr=0.00001, factor=0.25)\n    cluster_lr_scheduler = MyLRScheduler(cluster_optim, patience=15, min_lr=0.00001, factor=0.25)\n\n    loss_mat = np.zeros((100, 4, 2))\n\n    best_loss = 1e5\n    count = 0\n    for i in trange(100):\n        for step, (x_train, y_train) in enumerate(train_loader):\n            optimizer.zero_grad()\n            cluster_optim.zero_grad()\n\n            y_pred, probs = model.forward_pass(x_train)\n\n            loss_weights = class_weight(y_train)\n\n            common_loss = calc_pred_loss(y_train, y_pred, loss_weights)\n\n            enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n             + calc_l1_l2_loss(part=model.Encoder) \n            enc_loss.backward(retain_graph=True, inputs=list(model.Encoder.parameters()))\n\n            idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n            + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n            idnetifier_loss.backward(retain_graph=True, inputs=list(model.Identifier.parameters()))\n\n            pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n            pred_loss.backward(retain_graph=True, inputs=list(model.Predictor.parameters()))\n\n            clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n            clus_loss.backward(inputs=model.cluster_rep_set)\n\n            optimizer.step()\n            cluster_optim.step()\n\n            loss_mat[i, 0, 0] += enc_loss.item()\n            loss_mat[i, 1, 0] += idnetifier_loss.item()\n            loss_mat[i, 2, 0] += pred_loss.item()\n            loss_mat[i, 3, 0] += clus_loss.item()\n\n        with torch.no_grad():\n            for step, (x_val, y_val) in enumerate(val_loader):\n                y_pred, probs = model.forward_pass(x_val)\n\n                loss_weights = class_weight(y_val)\n\n                common_loss = calc_pred_loss(y_val, y_pred, loss_weights)\n\n                enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n                 + calc_l1_l2_loss(part=model.Encoder) \n\n                idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n                + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n\n                pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n\n                clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n\n                loss_mat[i, 0, 1] += enc_loss.item()\n                loss_mat[i, 1, 1] += idnetifier_loss.item()\n                loss_mat[i, 2, 1] += pred_loss.item()\n                loss_mat[i, 3, 1] += clus_loss.item()\n\n            if i >= 30:\n                if loss_mat[i, 0, 1] < best_loss:\n                    count = 0\n                    best_loss = loss_mat[i, 0, 1]\n                    torch.save(model.state_dict(), './best_model')\n                else:\n                    count += 1\n                    if count >= 50:\n                        model.load_state_dict(torch.load('./best_model'))\n        lr_scheduler.step(loss_mat[i, 0, 1])\n        cluster_lr_scheduler.step(loss_mat[i, 0, 1])\n\n#     print(calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3]), calc_l1_l2_loss(part=model.Encoder) + calc_l1_l2_loss(layers=[model.Identifier.fc2]))\n\n    model.load_state_dict(torch.load('./best_model'))\n\n    real, preds = [], []\n    with torch.no_grad():\n        for _, (x, y) in enumerate(test_loader):\n            y_pred, _ = model.forward_pass(x)\n            preds.extend(list(y_pred.cpu().detach().numpy()))\n            real.extend(list(y.cpu().detach().numpy()))\n\n    auc = roc_auc_score(real, preds, average=None)\n\n    labels_true, labels_pred = np.argmax(real, axis=1), np.argmax(preds, axis=1)\n\n    # Compute F1\n    f1 = f1_score(labels_true, labels_pred, average=None)\n\n    # Compute Recall\n    rec = recall_score(labels_true, labels_pred, average=None)\n\n    # Compute NMI\n    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n\n    print(f'AUCROC: \\t{auc.mean():.5f}, \\t{auc}')\n    print(f'F1-score: \\t{f1.mean():.5f}, \\t{f1}')\n    print(f'Recall: \\t{rec.mean():.5f}, \\t{rec}')\n    print(f'NMI: \\t\\t{nmi:.5f}')\n    \n    results[index, 0] = auc.mean()\n    results[index, 1] = f1.mean()\n    results[index, 2] = rec.mean()\n    results[index, 3] = nmi","metadata":{"execution":{"iopub.status.busy":"2023-05-01T23:57:57.663970Z","iopub.execute_input":"2023-05-01T23:57:57.664302Z","iopub.status.idle":"2023-05-02T00:13:51.820218Z","shell.execute_reply.started":"2023-05-01T23:57:57.664274Z","shell.execute_reply":"2023-05-02T00:13:51.819212Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:12<00:00, 632.52it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:14<00:14,  3.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:56<00:00,  1.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.77337, \t[0.87801372 0.78423927 0.75752452 0.67370918]\nF1-score: \t0.28072, \t[0.         0.47262248 0.65024631 0.        ]\nRecall: \t0.34648, \t[0.         0.8803681  0.50553191 0.        ]\nNMI: \t\t0.09244\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:13<00:00, 587.82it/s]\n 50%|█████     | 50/100 [00:15<00:15,  3.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:15<00:15,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [01:05<00:00,  1.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.78254, \t[0.88453937 0.78543456 0.74934337 0.71084645]\nF1-score: \t0.33899, \t[0.         0.50728863 0.84865209 0.        ]\nRecall: \t0.34610, \t[0.         0.53374233 0.8506383  0.        ]\nNMI: \t\t0.11785\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:12<00:00, 630.28it/s]\n 50%|█████     | 50/100 [00:14<00:14,  3.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:12<00:12,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.79506, \t[0.8972068  0.79897557 0.76723754 0.71681118]\nF1-score: \t0.34045, \t[0.         0.52589641 0.83591872 0.        ]\nRecall: \t0.35535, \t[0.         0.60736196 0.81404255 0.        ]\nNMI: \t\t0.11889\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 727.86it/s]\n 50%|█████     | 50/100 [00:14<00:14,  3.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:12<00:12,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.76984, \t[0.88031689 0.79648584 0.77335099 0.62918821]\nF1-score: \t0.31292, \t[0.         0.50793651 0.74375624 0.        ]\nRecall: \t0.36086, \t[0.         0.80981595 0.63361702 0.        ]\nNMI: \t\t0.10792\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 724.27it/s]\n 50%|█████     | 50/100 [00:14<00:14,  3.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:12<00:12,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.77188, \t[0.90392029 0.77566572 0.75097826 0.65695633]\nF1-score: \t0.33523, \t[0.         0.49857955 0.84235294 0.        ]\nRecall: \t0.34405, \t[0.         0.53834356 0.83787234 0.        ]\nNMI: \t\t0.10775\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:11<00:00, 697.72it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.70530, \t[0.79433192 0.72972417 0.71580348 0.58132733]\nF1-score: \t0.29846, \t[0.         0.4903975  0.70343392 0.        ]\nRecall: \t0.35434, \t[0.         0.84202454 0.57531915 0.        ]\nNMI: \t\t0.10191\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 722.75it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.78450, \t[0.91149951 0.78422632 0.75711587 0.68514229]\nF1-score: \t0.29423, \t[0.         0.48701013 0.68992655 0.        ]\nRecall: \t0.35193, \t[0.         0.84815951 0.55957447 0.        ]\nNMI: \t\t0.10370\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 710.62it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.77756, \t[0.91736361 0.77453451 0.76048549 0.6578481 ]\nF1-score: \t0.30760, \t[0.         0.50094162 0.72947714 0.        ]\nRecall: \t0.35760, \t[0.         0.81595092 0.61446809 0.        ]\nNMI: \t\t0.10547\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:11<00:00, 689.38it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.78638, \t[0.91034793 0.79900083 0.77334488 0.66282572]\nF1-score: \t0.34174, \t[0.         0.53725736 0.82969238 0.        ]\nRecall: \t0.36247, \t[0.         0.65797546 0.79191489 0.        ]\nNMI: \t\t0.12944\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:11<00:00, 696.55it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:53<00:00,  1.89it/s]","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.76785, \t[0.90296472 0.77128549 0.74813051 0.64903701]\nF1-score: \t0.32843, \t[0.         0.50537634 0.80834656 0.        ]\nRecall: \t0.35177, \t[0.         0.64877301 0.75829787 0.        ]\nNMI: \t\t0.10231\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"for m, u, std in zip(metrics, results.mean(axis=0), results.std(axis=0)):\n    print(f'{m}: {u:.3f} ({std:.3f})')","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:14:04.670428Z","iopub.execute_input":"2023-05-02T00:14:04.670813Z","iopub.status.idle":"2023-05-02T00:14:04.677223Z","shell.execute_reply.started":"2023-05-02T00:14:04.670780Z","shell.execute_reply":"2023-05-02T00:14:04.676150Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"AUC: 0.771 (0.023)\nF1 score: 0.318 (0.021)\nRecall: 0.353 (0.006)\nNMI: 0.109 (0.010)\n","output_type":"stream"}]},{"cell_type":"code","source":"results = np.zeros((len(seeds), 4))\nfor index, SEED in enumerate(seeds):\n    torch.random.manual_seed(SEED)\n    np.random.seed(SEED)\n    random.seed(SEED)\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    dataset = CustomDataset(time_range=(0, 10))\n\n    # Stratified Sampling for train and val\n    train_idx, test_idx = train_test_split(np.arange(len(dataset)),\n                                                test_size=0.4,\n                                                random_state=SEED,\n                                                shuffle=True,\n                                                stratify=np.argmax(dataset.y,axis=-1))\n\n    # Subset dataset for train and val\n    train_val_dataset = dataset.get_subset(train_idx)\n    test_dataset = dataset.get_subset(test_idx)\n\n    train_idx,  val_idx = train_test_split(np.arange(len(train_val_dataset)),\n                                                test_size=0.4,\n                                                random_state=SEED,\n                                                shuffle=True,\n                                                stratify=np.argmax(train_val_dataset.y,axis=-1))\n\n    train_dataset = train_val_dataset.get_subset(train_idx)\n    val_dataset = train_val_dataset.get_subset(val_idx)\n\n    train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset)\n\n    model = CamelotModel(input_shape=(train_dataset.x.shape[1], train_dataset.x.shape[2]), seed=SEED, num_clusters=10, latent_dim=64, beta=0)\n    model = model.to(device)\n\n    train_x = torch.tensor(train_dataset.x).to(device)\n    train_y = torch.tensor(train_dataset.y).to(device)\n    val_x = torch.tensor(val_dataset.x).to(device)\n    val_y = torch.tensor(val_dataset.y).to(device)\n\n    model.initialize((train_x, train_y), (val_x, val_y))\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    cluster_optim = torch.optim.Adam([model.cluster_rep_set], lr=0.001)\n\n    lr_scheduler = MyLRScheduler(optimizer, patience=15, min_lr=0.00001, factor=0.25)\n    cluster_lr_scheduler = MyLRScheduler(cluster_optim, patience=15, min_lr=0.00001, factor=0.25)\n\n    loss_mat = np.zeros((100, 4, 2))\n\n    best_loss = 1e5\n    count = 0\n    for i in trange(100):\n        for step, (x_train, y_train) in enumerate(train_loader):\n            optimizer.zero_grad()\n            cluster_optim.zero_grad()\n\n            y_pred, probs = model.forward_pass(x_train)\n\n            loss_weights = class_weight(y_train)\n\n            common_loss = calc_pred_loss(y_train, y_pred, loss_weights)\n\n            enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n             + calc_l1_l2_loss(part=model.Encoder) \n            enc_loss.backward(retain_graph=True, inputs=list(model.Encoder.parameters()))\n\n            idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n            + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n            idnetifier_loss.backward(retain_graph=True, inputs=list(model.Identifier.parameters()))\n\n            pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n            pred_loss.backward(retain_graph=True, inputs=list(model.Predictor.parameters()))\n\n            clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n            clus_loss.backward(inputs=model.cluster_rep_set)\n\n            optimizer.step()\n            cluster_optim.step()\n\n            loss_mat[i, 0, 0] += enc_loss.item()\n            loss_mat[i, 1, 0] += idnetifier_loss.item()\n            loss_mat[i, 2, 0] += pred_loss.item()\n            loss_mat[i, 3, 0] += clus_loss.item()\n\n        with torch.no_grad():\n            for step, (x_val, y_val) in enumerate(val_loader):\n                y_pred, probs = model.forward_pass(x_val)\n\n                loss_weights = class_weight(y_val)\n\n                common_loss = calc_pred_loss(y_val, y_pred, loss_weights)\n\n                enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n                 + calc_l1_l2_loss(part=model.Encoder) \n\n                idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n                + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n\n                pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n\n                clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n\n                loss_mat[i, 0, 1] += enc_loss.item()\n                loss_mat[i, 1, 1] += idnetifier_loss.item()\n                loss_mat[i, 2, 1] += pred_loss.item()\n                loss_mat[i, 3, 1] += clus_loss.item()\n\n            if i >= 30:\n                if loss_mat[i, 0, 1] < best_loss:\n                    count = 0\n                    best_loss = loss_mat[i, 0, 1]\n                    torch.save(model.state_dict(), './best_model')\n                else:\n                    count += 1\n                    if count >= 50:\n                        model.load_state_dict(torch.load('./best_model'))\n        lr_scheduler.step(loss_mat[i, 0, 1])\n        cluster_lr_scheduler.step(loss_mat[i, 0, 1])\n\n#     print(calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3]), calc_l1_l2_loss(part=model.Encoder) + calc_l1_l2_loss(layers=[model.Identifier.fc2]))\n\n    model.load_state_dict(torch.load('./best_model'))\n\n    real, preds = [], []\n    with torch.no_grad():\n        for _, (x, y) in enumerate(test_loader):\n            y_pred, _ = model.forward_pass(x)\n            preds.extend(list(y_pred.cpu().detach().numpy()))\n            real.extend(list(y.cpu().detach().numpy()))\n\n    auc = roc_auc_score(real, preds, average=None)\n\n    labels_true, labels_pred = np.argmax(real, axis=1), np.argmax(preds, axis=1)\n\n    # Compute F1\n    f1 = f1_score(labels_true, labels_pred, average=None)\n\n    # Compute Recall\n    rec = recall_score(labels_true, labels_pred, average=None)\n\n    # Compute NMI\n    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n\n    print(f'AUCROC: \\t{auc.mean():.5f}, \\t{auc}')\n    print(f'F1-score: \\t{f1.mean():.5f}, \\t{f1}')\n    print(f'Recall: \\t{rec.mean():.5f}, \\t{rec}')\n    print(f'NMI: \\t\\t{nmi:.5f}')\n    \n    results[index, 0] = auc.mean()\n    results[index, 1] = f1.mean()\n    results[index, 2] = rec.mean()\n    results[index, 3] = nmi","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:15:06.703128Z","iopub.execute_input":"2023-05-02T00:15:06.703468Z","iopub.status.idle":"2023-05-02T00:30:32.873535Z","shell.execute_reply.started":"2023-05-02T00:15:06.703439Z","shell.execute_reply":"2023-05-02T00:30:32.872601Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:11<00:00, 662.78it/s]\n 50%|█████     | 50/100 [00:14<00:14,  3.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:12<00:12,  3.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.76733, \t[0.86754329 0.77836003 0.75103647 0.67239117]\nF1-score: \t0.33383, \t[0.         0.49459265 0.8407155  0.        ]\nRecall: \t0.34152, \t[0.         0.52607362 0.84       0.        ]\nNMI: \t\t0.10798\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 726.57it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.77330, \t[0.8613035  0.78128133 0.74634194 0.7042872 ]\nF1-score: \t0.28195, \t[0.         0.47196653 0.65583536 0.        ]\nRecall: \t0.34509, \t[0.         0.86503067 0.51531915 0.        ]\nNMI: \t\t0.09085\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 709.26it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.79171, \t[0.89135903 0.79909175 0.76810752 0.70826369]\nF1-score: \t0.33788, \t[0.         0.51111111 0.84040491 0.        ]\nRecall: \t0.34866, \t[0.         0.56441718 0.83021277 0.        ]\nNMI: \t\t0.11117\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 718.98it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.75960, \t[0.87720516 0.76079271 0.77068894 0.62970981]\nF1-score: \t0.32774, \t[0.         0.52421959 0.78674556 0.        ]\nRecall: \t0.36354, \t[0.         0.74693252 0.70723404 0.        ]\nNMI: \t\t0.11623\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 708.20it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.76947, \t[0.91386802 0.76688569 0.74496173 0.65214977]\nF1-score: \t0.32637, \t[0.         0.50293772 0.80255649 0.        ]\nRecall: \t0.35113, \t[0.         0.65644172 0.74808511 0.        ]\nNMI: \t\t0.10022\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:11<00:00, 691.61it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.75183, \t[0.90486769 0.74914568 0.73146433 0.62185218]\nF1-score: \t0.28803, \t[0.         0.46264626 0.68945869 0.        ]\nRecall: \t0.33868, \t[0.         0.78834356 0.56638298 0.        ]\nNMI: \t\t0.07525\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 702.60it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.74883, \t[0.83957857 0.75443043 0.73177082 0.66955322]\nF1-score: \t0.33783, \t[0.         0.49573974 0.85559265 0.        ]\nRecall: \t0.34078, \t[0.         0.49079755 0.87234043 0.        ]\nNMI: \t\t0.11604\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 700.60it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:12<00:12,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.73254, \t[0.85329958 0.73570033 0.71219897 0.6289807 ]\nF1-score: \t0.26343, \t[0.         0.45153756 0.60216278 0.        ]\nRecall: \t0.33495, \t[0.         0.88957055 0.45021277 0.        ]\nNMI: \t\t0.07883\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 707.57it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:12<00:12,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:53<00:00,  1.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.78649, \t[0.92156158 0.78833882 0.76922461 0.66684708]\nF1-score: \t0.34197, \t[0.         0.53537285 0.83252105 0.        ]\nRecall: \t0.36094, \t[0.         0.64417178 0.79957447 0.        ]\nNMI: \t\t0.13199\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 726.08it/s]\n 50%|█████     | 50/100 [00:14<00:14,  3.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:12<00:12,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.90it/s]","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.77283, \t[0.88434335 0.78302534 0.74520563 0.67876533]\nF1-score: \t0.33541, \t[0.         0.50140845 0.84023161 0.        ]\nRecall: \t0.34491, \t[0.         0.54601227 0.83361702 0.        ]\nNMI: \t\t0.10777\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"for m, u, std in zip(metrics, results.mean(axis=0), results.std(axis=0)):\n    print(f'{m}: {u:.3f} ({std:.3f})')","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:31:27.566515Z","iopub.execute_input":"2023-05-02T00:31:27.567195Z","iopub.status.idle":"2023-05-02T00:31:27.572137Z","shell.execute_reply.started":"2023-05-02T00:31:27.567159Z","shell.execute_reply":"2023-05-02T00:31:27.571249Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"AUC: 0.765 (0.017)\nF1 score: 0.317 (0.027)\nRecall: 0.347 (0.009)\nNMI: 0.104 (0.017)\n","output_type":"stream"}]},{"cell_type":"code","source":"results = np.zeros((len(seeds), 4))\nfor index, SEED in enumerate(seeds):\n    torch.random.manual_seed(SEED)\n    np.random.seed(SEED)\n    random.seed(SEED)\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    dataset = CustomDataset(time_range=(0, 10))\n\n    # Stratified Sampling for train and val\n    train_idx, test_idx = train_test_split(np.arange(len(dataset)),\n                                                test_size=0.4,\n                                                random_state=SEED,\n                                                shuffle=True,\n                                                stratify=np.argmax(dataset.y,axis=-1))\n\n    # Subset dataset for train and val\n    train_val_dataset = dataset.get_subset(train_idx)\n    test_dataset = dataset.get_subset(test_idx)\n\n    train_idx,  val_idx = train_test_split(np.arange(len(train_val_dataset)),\n                                                test_size=0.4,\n                                                random_state=SEED,\n                                                shuffle=True,\n                                                stratify=np.argmax(train_val_dataset.y,axis=-1))\n\n    train_dataset = train_val_dataset.get_subset(train_idx)\n    val_dataset = train_val_dataset.get_subset(val_idx)\n\n    train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset)\n\n    model = CamelotModel(input_shape=(train_dataset.x.shape[1], train_dataset.x.shape[2]), seed=SEED, num_clusters=10, latent_dim=64, beta=0, alpha=0)\n    model = model.to(device)\n\n    train_x = torch.tensor(train_dataset.x).to(device)\n    train_y = torch.tensor(train_dataset.y).to(device)\n    val_x = torch.tensor(val_dataset.x).to(device)\n    val_y = torch.tensor(val_dataset.y).to(device)\n\n    model.initialize((train_x, train_y), (val_x, val_y))\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    cluster_optim = torch.optim.Adam([model.cluster_rep_set], lr=0.001)\n\n    lr_scheduler = MyLRScheduler(optimizer, patience=15, min_lr=0.00001, factor=0.25)\n    cluster_lr_scheduler = MyLRScheduler(cluster_optim, patience=15, min_lr=0.00001, factor=0.25)\n\n    loss_mat = np.zeros((100, 4, 2))\n\n    best_loss = 1e5\n    count = 0\n    for i in trange(100):\n        for step, (x_train, y_train) in enumerate(train_loader):\n            optimizer.zero_grad()\n            cluster_optim.zero_grad()\n\n            y_pred, probs = model.forward_pass(x_train)\n\n            loss_weights = class_weight(y_train)\n\n            common_loss = calc_pred_loss(y_train, y_pred, loss_weights)\n\n            enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n             + calc_l1_l2_loss(part=model.Encoder) \n            enc_loss.backward(retain_graph=True, inputs=list(model.Encoder.parameters()))\n\n            idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n            + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n            idnetifier_loss.backward(retain_graph=True, inputs=list(model.Identifier.parameters()))\n\n            pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n            pred_loss.backward(retain_graph=True, inputs=list(model.Predictor.parameters()))\n\n            clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n            clus_loss.backward(inputs=model.cluster_rep_set)\n\n            optimizer.step()\n            cluster_optim.step()\n\n            loss_mat[i, 0, 0] += enc_loss.item()\n            loss_mat[i, 1, 0] += idnetifier_loss.item()\n            loss_mat[i, 2, 0] += pred_loss.item()\n            loss_mat[i, 3, 0] += clus_loss.item()\n\n        with torch.no_grad():\n            for step, (x_val, y_val) in enumerate(val_loader):\n                y_pred, probs = model.forward_pass(x_val)\n\n                loss_weights = class_weight(y_val)\n\n                common_loss = calc_pred_loss(y_val, y_pred, loss_weights)\n\n                enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n                 + calc_l1_l2_loss(part=model.Encoder) \n\n                idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n                + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n\n                pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n\n                clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n\n                loss_mat[i, 0, 1] += enc_loss.item()\n                loss_mat[i, 1, 1] += idnetifier_loss.item()\n                loss_mat[i, 2, 1] += pred_loss.item()\n                loss_mat[i, 3, 1] += clus_loss.item()\n\n            if i >= 30:\n                if loss_mat[i, 0, 1] < best_loss:\n                    count = 0\n                    best_loss = loss_mat[i, 0, 1]\n                    torch.save(model.state_dict(), './best_model')\n                else:\n                    count += 1\n                    if count >= 50:\n                        model.load_state_dict(torch.load('./best_model'))\n        lr_scheduler.step(loss_mat[i, 0, 1])\n        cluster_lr_scheduler.step(loss_mat[i, 0, 1])\n\n#     print(calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3]), calc_l1_l2_loss(part=model.Encoder) + calc_l1_l2_loss(layers=[model.Identifier.fc2]))\n\n    model.load_state_dict(torch.load('./best_model'))\n\n    real, preds = [], []\n    with torch.no_grad():\n        for _, (x, y) in enumerate(test_loader):\n            y_pred, _ = model.forward_pass(x)\n            preds.extend(list(y_pred.cpu().detach().numpy()))\n            real.extend(list(y.cpu().detach().numpy()))\n\n    auc = roc_auc_score(real, preds, average=None)\n\n    labels_true, labels_pred = np.argmax(real, axis=1), np.argmax(preds, axis=1)\n\n    # Compute F1\n    f1 = f1_score(labels_true, labels_pred, average=None)\n\n    # Compute Recall\n    rec = recall_score(labels_true, labels_pred, average=None)\n\n    # Compute NMI\n    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n\n    print(f'AUCROC: \\t{auc.mean():.5f}, \\t{auc}')\n    print(f'F1-score: \\t{f1.mean():.5f}, \\t{f1}')\n    print(f'Recall: \\t{rec.mean():.5f}, \\t{rec}')\n    print(f'NMI: \\t\\t{nmi:.5f}')\n    \n    results[index, 0] = auc.mean()\n    results[index, 1] = f1.mean()\n    results[index, 2] = rec.mean()\n    results[index, 3] = nmi","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:33:17.501967Z","iopub.execute_input":"2023-05-02T00:33:17.502324Z","iopub.status.idle":"2023-05-02T00:48:42.478518Z","shell.execute_reply.started":"2023-05-02T00:33:17.502294Z","shell.execute_reply":"2023-05-02T00:48:42.477713Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:12<00:00, 640.32it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.75846, \t[0.86568115 0.77298751 0.75408039 0.64108964]\nF1-score: \t0.29019, \t[0.         0.47993095 0.68082847 0.        ]\nRecall: \t0.34957, \t[0.         0.85276074 0.54553191 0.        ]\nNMI: \t\t0.09296\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:11<00:00, 692.29it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.76823, \t[0.8677883  0.75090831 0.74081293 0.7134292 ]\nF1-score: \t0.32112, \t[0.         0.4909621  0.79349817 0.        ]\nRecall: \t0.34579, \t[0.         0.64570552 0.73744681 0.        ]\nNMI: \t\t0.09323\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:11<00:00, 698.65it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:12<00:12,  3.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.78961, \t[0.88619732 0.79715642 0.76605815 0.70903487]\nF1-score: \t0.33811, \t[0.         0.50681981 0.845629   0.        ]\nRecall: \t0.34631, \t[0.         0.54141104 0.84382979 0.        ]\nNMI: \t\t0.11115\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 731.20it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:12<00:12,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.76238, \t[0.88584613 0.76743124 0.75367494 0.64255067]\nF1-score: \t0.32349, \t[0.         0.51809124 0.77586207 0.        ]\nRecall: \t0.36176, \t[0.         0.75766871 0.6893617  0.        ]\nNMI: \t\t0.11198\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 726.84it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:12<00:12,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.77184, \t[0.91246325 0.77239491 0.74987746 0.6526265 ]\nF1-score: \t0.32470, \t[0.         0.50343249 0.79538639 0.        ]\nRecall: \t0.35212, \t[0.         0.67484663 0.73361702 0.        ]\nNMI: \t\t0.10018\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 721.34it/s]\n 50%|█████     | 50/100 [00:14<00:14,  3.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:12<00:12,  3.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.74490, \t[0.87101437 0.74903076 0.73178479 0.62778046]\nF1-score: \t0.31412, \t[0.         0.49534643 0.76114726 0.        ]\nRecall: \t0.35164, \t[0.         0.73466258 0.67191489 0.        ]\nNMI: \t\t0.09452\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 729.75it/s]\n 50%|█████     | 50/100 [00:14<00:14,  3.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:12<00:12,  3.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.75511, \t[0.86612218 0.75408566 0.73148441 0.66873437]\nF1-score: \t0.33747, \t[0.         0.49041534 0.85944939 0.        ]\nRecall: \t0.33857, \t[0.         0.4708589  0.88340426 0.        ]\nNMI: \t\t0.11696\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 728.05it/s]\n 50%|█████     | 50/100 [00:14<00:14,  3.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:12<00:12,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.78484, \t[0.93335511 0.77795118 0.76374945 0.6643148 ]\nF1-score: \t0.33817, \t[0.         0.53524492 0.81741892 0.        ]\nRecall: \t0.36348, \t[0.         0.68711656 0.76680851 0.        ]\nNMI: \t\t0.13032\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 724.72it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.78245, \t[0.92308069 0.78952907 0.77596793 0.64122705]\nF1-score: \t0.33205, \t[0.         0.52249135 0.80570246 0.        ]\nRecall: \t0.36008, \t[0.         0.69478528 0.74553191 0.        ]\nNMI: \t\t0.11790\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:11<00:00, 689.16it/s]\n 50%|█████     | 50/100 [00:13<00:13,  3.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:13<00:13,  3.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:52<00:00,  1.89it/s]","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.76318, \t[0.90655831 0.75849588 0.72001746 0.6676463 ]\nF1-score: \t0.31040, \t[0.         0.51346274 0.66734694 0.0608    ]\nRecall: \t0.37687, \t[0.         0.62883436 0.55659574 0.3220339 ]\nNMI: \t\t0.09700\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"for m, u, std in zip(metrics, results.mean(axis=0), results.std(axis=0)):\n    print(f'{m}: {u:.3f} ({std:.3f})')","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:48:48.358610Z","iopub.execute_input":"2023-05-02T00:48:48.358984Z","iopub.status.idle":"2023-05-02T00:48:48.368046Z","shell.execute_reply.started":"2023-05-02T00:48:48.358954Z","shell.execute_reply":"2023-05-02T00:48:48.367173Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"AUC: 0.768 (0.013)\nF1 score: 0.323 (0.014)\nRecall: 0.355 (0.010)\nNMI: 0.107 (0.012)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FeatTimeAttention(nn.Module):\n    def __init__(self, latent_dim, input_shape):\n        super().__init__()\n\n        self.device = torch.device(\n            'cuda' if torch.cuda.is_available() else 'cpu')\n\n        self.latent_dim = latent_dim\n        T, D_f = input_shape\n        # Define Kernel and Bias for Feature Projection\n        self.kernel = torch.zeros(\n            (1, 1, D_f, self.latent_dim), requires_grad=True).to(self.device)\n        nn.init.xavier_uniform_(self.kernel)\n        self.bias = torch.zeros(\n            (1, 1, D_f, self.latent_dim), requires_grad=True).to(self.device)\n        nn.init.uniform_(self.bias)\n\n        # Define Time aggregation weights for averaging over time.\n        self.unnorm_beta = torch.zeros((1, T, 1), requires_grad=True)\n        nn.init.uniform_(self.unnorm_beta)\n\n    def forward(self, x, latent):\n        o_hat, _ = self.generate_latent_approx(x, latent)\n        weights = self.calc_weights(self.unnorm_beta)\n        # print(o_hat.shape, weights.shape)\n        return torch.sum(torch.mul(o_hat.to(self.device), weights.to(self.device)), dim=1)\n\n    def generate_latent_approx(self, x, latent):\n        features = torch.mul(x.unsqueeze(-1), self.kernel) + self.bias\n        features = F.relu(features)\n\n        # calculate the score\n        X_T, X = features, features.transpose(2, 3)\n        # print(X_T.shape, X.shape)\n        X_T_X_inv = torch.inverse(torch.matmul(X_T, X))\n        # print(X_T.shape, latent.unsqueeze(-1).shape)\n        X_T_y = torch.matmul(X_T, latent.unsqueeze(-1))\n\n        score_hat = torch.matmul(X_T_X_inv, X_T_y)\n        scores = torch.squeeze(score_hat)\n\n        # print(scores.unsqueeze(-1).shape, features.shape)\n        o_hat = torch.sum(torch.mul(scores.unsqueeze(-1), features), dim=2)\n\n        return o_hat, scores\n\n    def calc_weights(self, x):\n        abs_x = torch.abs(x)\n        return abs_x / torch.sum(abs_x, dim=1)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_shape, attention_hidden_dim, latent_dim, dropout):\n        super().__init__()\n        self.lstm1 = nn.LSTM(input_size=input_shape[1],\n                             hidden_size=attention_hidden_dim,\n                             num_layers=2,\n                             dropout=dropout,\n                             batch_first=True)\n        self.lstm2 = nn.LSTM(input_size=attention_hidden_dim,\n                             hidden_size=latent_dim,\n                             num_layers=1,\n                             batch_first=True)\n        self.attention = FeatTimeAttention(latent_dim, input_shape)\n        self.fc = nn.Linear(10*(input_shape[1]+latent_dim), latent_dim)\n\n    def forward(self, x):\n        latent_rep, _ = self.lstm1(x)\n        latent_rep, _ = self.lstm2(latent_rep)\n        x_latent = torch.cat((x, latent_rep),dim=2)\n        x_latent_flat = torch.flatten(x_latent, start_dim=1)\n        output = self.fc(x_latent_flat)\n        return output\n\n\nclass Identifier(nn.Module):\n    def __init__(self, input_dim, mlp_hidden_dim, dropout, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, mlp_hidden_dim)\n        self.sigmoid1 = nn.Sigmoid()\n\n        self.fc2 = nn.Linear(mlp_hidden_dim, mlp_hidden_dim)\n        self.sigmoid2 = nn.Sigmoid()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.fc4 = nn.Linear(mlp_hidden_dim, output_dim)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.sigmoid1(x)\n\n        x = self.fc2(x)\n        x = self.sigmoid2(x)\n        x = self.dropout1(x)\n\n        x = self.fc4(x)\n        x = self.softmax(x)\n        return x\n\n\nclass Predictor(nn.Module):\n    def __init__(self, input_dim, mlp_hidden_dim, dropout, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, mlp_hidden_dim)\n        self.sigmoid1 = nn.Sigmoid()\n\n        self.fc2 = nn.Linear(mlp_hidden_dim, mlp_hidden_dim)\n        self.sigmoid2 = nn.Sigmoid()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.fc3 = nn.Linear(mlp_hidden_dim, mlp_hidden_dim)\n        self.sigmoid3 = nn.Sigmoid()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.fc4 = nn.Linear(mlp_hidden_dim, output_dim)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.sigmoid1(x)\n\n        x = self.fc2(x)\n        x = self.sigmoid2(x)\n        x = self.dropout1(x)\n\n        x = self.fc3(x)\n        x = self.sigmoid3(x)\n        x = self.dropout2(x)\n\n        x = self.fc4(x)\n        x = self.softmax(x)\n        return x\n\n\nclass MyLRScheduler():\n    def __init__(self, optimizer, patience, min_lr, factor):\n        self.optimizer = optimizer\n        self.patience = patience\n        self.min_lr = min_lr\n        self.factor = factor\n        self.wait = 0\n        self.best_loss = float('inf')\n\n    def step(self, val_loss):\n        if val_loss < self.best_loss:\n            self.best_loss = val_loss\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.wait = 0\n                for param_group in self.optimizer.param_groups:\n                    old_lr = param_group['lr']\n                    new_lr = max(old_lr * self.factor, self.min_lr)\n                    param_group['lr'] = new_lr\n\n\ndef calc_l1_l2_loss(part=None, layers=None):\n    para = []\n    if part:\n        for parameter in part.parameters():\n            para.append(parameter.view(-1))\n        parameters = torch.cat(para)\n    if layers:\n        for layer in layers:\n            para.extend(layer.parameters())\n        parameters = torch.cat([p.view(-1) for p in para])\n    return 1e-30 * torch.abs(parameters).sum() + 1e-30 * torch.square(parameters).sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:49:42.289758Z","iopub.execute_input":"2023-05-02T00:49:42.290105Z","iopub.status.idle":"2023-05-02T00:49:42.313517Z","shell.execute_reply.started":"2023-05-02T00:49:42.290077Z","shell.execute_reply":"2023-05-02T00:49:42.312676Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"results = np.zeros((len(seeds), 4))\nfor index, SEED in enumerate(seeds):\n    torch.random.manual_seed(SEED)\n    np.random.seed(SEED)\n    random.seed(SEED)\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    dataset = CustomDataset(time_range=(0, 10))\n\n    # Stratified Sampling for train and val\n    train_idx, test_idx = train_test_split(np.arange(len(dataset)),\n                                                test_size=0.4,\n                                                random_state=SEED,\n                                                shuffle=True,\n                                                stratify=np.argmax(dataset.y,axis=-1))\n\n    # Subset dataset for train and val\n    train_val_dataset = dataset.get_subset(train_idx)\n    test_dataset = dataset.get_subset(test_idx)\n\n    train_idx,  val_idx = train_test_split(np.arange(len(train_val_dataset)),\n                                                test_size=0.4,\n                                                random_state=SEED,\n                                                shuffle=True,\n                                                stratify=np.argmax(train_val_dataset.y,axis=-1))\n\n    train_dataset = train_val_dataset.get_subset(train_idx)\n    val_dataset = train_val_dataset.get_subset(val_idx)\n\n    train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset)\n\n    model = CamelotModel(input_shape=(train_dataset.x.shape[1], train_dataset.x.shape[2]), seed=SEED, num_clusters=10, latent_dim=64)\n    model = model.to(device)\n\n    train_x = torch.tensor(train_dataset.x).to(device)\n    train_y = torch.tensor(train_dataset.y).to(device)\n    val_x = torch.tensor(val_dataset.x).to(device)\n    val_y = torch.tensor(val_dataset.y).to(device)\n\n    model.initialize((train_x, train_y), (val_x, val_y))\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    cluster_optim = torch.optim.Adam([model.cluster_rep_set], lr=0.001)\n\n    lr_scheduler = MyLRScheduler(optimizer, patience=15, min_lr=0.00001, factor=0.25)\n    cluster_lr_scheduler = MyLRScheduler(cluster_optim, patience=15, min_lr=0.00001, factor=0.25)\n\n    loss_mat = np.zeros((100, 4, 2))\n\n    best_loss = 1e5\n    count = 0\n    for i in trange(100):\n        for step, (x_train, y_train) in enumerate(train_loader):\n            optimizer.zero_grad()\n            cluster_optim.zero_grad()\n\n            y_pred, probs = model.forward_pass(x_train)\n\n            loss_weights = class_weight(y_train)\n\n            common_loss = calc_pred_loss(y_train, y_pred, loss_weights)\n\n            enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n             + calc_l1_l2_loss(part=model.Encoder) \n            enc_loss.backward(retain_graph=True, inputs=list(model.Encoder.parameters()))\n\n            idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n            + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n            idnetifier_loss.backward(retain_graph=True, inputs=list(model.Identifier.parameters()))\n\n            pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n            pred_loss.backward(retain_graph=True, inputs=list(model.Predictor.parameters()))\n\n            clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n            clus_loss.backward(inputs=model.cluster_rep_set)\n\n            optimizer.step()\n            cluster_optim.step()\n\n            loss_mat[i, 0, 0] += enc_loss.item()\n            loss_mat[i, 1, 0] += idnetifier_loss.item()\n            loss_mat[i, 2, 0] += pred_loss.item()\n            loss_mat[i, 3, 0] += clus_loss.item()\n\n        with torch.no_grad():\n            for step, (x_val, y_val) in enumerate(val_loader):\n                y_pred, probs = model.forward_pass(x_val)\n\n                loss_weights = class_weight(y_val)\n\n                common_loss = calc_pred_loss(y_val, y_pred, loss_weights)\n\n                enc_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n                 + calc_l1_l2_loss(part=model.Encoder) \n\n                idnetifier_loss = common_loss + model.alpha * calc_dist_loss(probs) + \\\n                + calc_l1_l2_loss(layers=[model.Identifier.fc2])\n\n                pred_loss = common_loss + calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3])\n\n                clus_loss = common_loss + model.beta * calc_clus_loss(model.cluster_rep_set)\n\n                loss_mat[i, 0, 1] += enc_loss.item()\n                loss_mat[i, 1, 1] += idnetifier_loss.item()\n                loss_mat[i, 2, 1] += pred_loss.item()\n                loss_mat[i, 3, 1] += clus_loss.item()\n\n            if i >= 30:\n                if loss_mat[i, 0, 1] < best_loss:\n                    count = 0\n                    best_loss = loss_mat[i, 0, 1]\n                    torch.save(model.state_dict(), './best_model')\n                else:\n                    count += 1\n                    if count >= 50:\n                        model.load_state_dict(torch.load('./best_model'))\n        lr_scheduler.step(loss_mat[i, 0, 1])\n        cluster_lr_scheduler.step(loss_mat[i, 0, 1])\n\n#     print(calc_l1_l2_loss(layers=[model.Predictor.fc2, model.Predictor.fc3]), calc_l1_l2_loss(part=model.Encoder) + calc_l1_l2_loss(layers=[model.Identifier.fc2]))\n\n    model.load_state_dict(torch.load('./best_model'))\n\n    real, preds = [], []\n    with torch.no_grad():\n        for _, (x, y) in enumerate(test_loader):\n            y_pred, _ = model.forward_pass(x)\n            preds.extend(list(y_pred.cpu().detach().numpy()))\n            real.extend(list(y.cpu().detach().numpy()))\n\n    auc = roc_auc_score(real, preds, average=None)\n\n    labels_true, labels_pred = np.argmax(real, axis=1), np.argmax(preds, axis=1)\n\n    # Compute F1\n    f1 = f1_score(labels_true, labels_pred, average=None)\n\n    # Compute Recall\n    rec = recall_score(labels_true, labels_pred, average=None)\n\n    # Compute NMI\n    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n\n    print(f'AUCROC: \\t{auc.mean():.5f}, \\t{auc}')\n    print(f'F1-score: \\t{f1.mean():.5f}, \\t{f1}')\n    print(f'Recall: \\t{rec.mean():.5f}, \\t{rec}')\n    print(f'NMI: \\t\\t{nmi:.5f}')\n    \n    results[index, 0] = auc.mean()\n    results[index, 1] = f1.mean()\n    results[index, 2] = rec.mean()\n    results[index, 3] = nmi","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:49:48.188098Z","iopub.execute_input":"2023-05-02T00:49:48.188458Z","iopub.status.idle":"2023-05-02T01:03:47.371584Z","shell.execute_reply.started":"2023-05-02T00:49:48.188428Z","shell.execute_reply":"2023-05-02T01:03:47.370673Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 724.94it/s]\n 50%|█████     | 50/100 [00:11<00:11,  4.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:10<00:10,  4.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:48<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.74739, \t[0.82877328 0.77482939 0.7145033  0.67147136]\nF1-score: \t0.33127, \t[0.         0.52077238 0.80429813 0.        ]\nRecall: \t0.35776, \t[0.         0.68251534 0.74851064 0.        ]\nNMI: \t\t0.11608\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:11<00:00, 680.76it/s]\n 50%|█████     | 50/100 [00:11<00:11,  4.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:10<00:10,  4.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:50<00:00,  1.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.74947, \t[0.84624306 0.7394545  0.69009023 0.72210288]\nF1-score: \t0.32191, \t[0.         0.48447961 0.80315315 0.        ]\nRecall: \t0.34229, \t[0.         0.61042945 0.7587234  0.        ]\nNMI: \t\t0.08950\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 710.55it/s]\n 50%|█████     | 50/100 [00:11<00:11,  4.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:10<00:10,  4.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:49<00:00,  2.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.79067, \t[0.8895214  0.7793479  0.76392875 0.72987919]\nF1-score: \t0.30577, \t[0.         0.53669222 0.59365347 0.09271523]\nRecall: \t0.44864, \t[0.         0.75153374 0.44978723 0.59322034]\nNMI: \t\t0.11039\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 707.15it/s]\n 50%|█████     | 50/100 [00:11<00:11,  4.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:10<00:10,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:49<00:00,  2.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.78227, \t[0.9286671  0.77446379 0.78896237 0.63697574]\nF1-score: \t0.36807, \t[0.22916667 0.49012658 0.75297619 0.        ]\nRecall: \t0.48457, \t[0.55       0.74233129 0.64595745 0.        ]\nNMI: \t\t0.13074\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:11<00:00, 689.20it/s]\n 50%|█████     | 50/100 [00:11<00:11,  4.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:10<00:10,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:49<00:00,  2.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.74888, \t[0.89362137 0.76307975 0.65459935 0.6842337 ]\nF1-score: \t0.33317, \t[0.         0.48291572 0.8497692  0.        ]\nRecall: \t0.33736, \t[0.         0.48773006 0.86170213 0.        ]\nNMI: \t\t0.10325\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 721.88it/s]\n 50%|█████     | 50/100 [00:11<00:11,  4.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:10<00:10,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:49<00:00,  2.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.79460, \t[0.92165959 0.81997912 0.79530314 0.64146541]\nF1-score: \t0.34689, \t[0.         0.56100478 0.82656994 0.        ]\nRecall: \t0.37377, \t[0.         0.71932515 0.77574468 0.        ]\nNMI: \t\t0.15096\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 704.38it/s]\n 50%|█████     | 50/100 [00:11<00:11,  4.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:10<00:10,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:49<00:00,  2.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.79979, \t[0.91344332 0.80936385 0.77216637 0.70417223]\nF1-score: \t0.35637, \t[0.11904762 0.53914067 0.69051322 0.07677543]\nRecall: \t0.46456, \t[0.25       0.70245399 0.56680851 0.33898305]\nNMI: \t\t0.12990\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:11<00:00, 680.38it/s]\n 50%|█████     | 50/100 [00:11<00:11,  4.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:10<00:10,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:50<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.77607, \t[0.92494283 0.75360546 0.76150304 0.66422787]\nF1-score: \t0.31610, \t[0.07174888 0.42314436 0.76948909 0.        ]\nRecall: \t0.40944, \t[0.4        0.55521472 0.68255319 0.        ]\nNMI: \t\t0.11068\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 709.67it/s]\n 50%|█████     | 50/100 [00:11<00:11,  4.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:10<00:10,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:49<00:00,  2.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.51137, \t[0.48918654 0.52599343 0.49838927 0.53191847]\nF1-score: \t0.21635, \t[0.         0.         0.86540232 0.        ]\nRecall: \t0.25000, \t[0. 0. 1. 0.]\nNMI: \t\t0.00000\n\nMIMIC data has been subsettted to the following features: \n ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'].\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7701/7701 [00:10<00:00, 712.24it/s]\n 50%|█████     | 50/100 [00:11<00:11,  4.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Encoder initialization done!\nKmeans initialization done!\nCluster initialization done!\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [00:10<00:10,  4.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Identifier initialization done!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:49<00:00,  2.02it/s]","output_type":"stream"},{"name":"stdout","text":"AUCROC: \t0.78165, \t[0.9263476  0.78865138 0.75518235 0.65642632]\nF1-score: \t0.32128, \t[0.         0.5242047  0.69802956 0.06289308]\nRecall: \t0.38081, \t[0.         0.58128834 0.60297872 0.33898305]\nNMI: \t\t0.11034\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"for m, u, std in zip(metrics, results.mean(axis=0), results.std(axis=0)):\n    print(f'{m}: {u:.3f} ({std:.3f})')","metadata":{"execution":{"iopub.status.busy":"2023-05-02T01:03:51.290141Z","iopub.execute_input":"2023-05-02T01:03:51.290488Z","iopub.status.idle":"2023-05-02T01:03:51.299541Z","shell.execute_reply.started":"2023-05-02T01:03:51.290460Z","shell.execute_reply":"2023-05-02T01:03:51.298706Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"AUC: 0.748 (0.081)\nF1 score: 0.322 (0.039)\nRecall: 0.385 (0.066)\nNMI: 0.105 (0.039)\n","output_type":"stream"}]}]}